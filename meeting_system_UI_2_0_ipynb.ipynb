{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# âš™ï¸ å®‰è£ç›¸å®¹ç‰ˆæœ¬ï¼ˆé™ torch / torchaudio / pyannoteï¼‰\n",
        "!pip install torch==1.10.0+cu113 torchaudio==0.10.0 -f https://download.pytorch.org/whl/torch_stable.html -q\n",
        "!pip install pytorch-lightning==1.6.5 -q\n",
        "!pip install git+https://github.com/m-bain/whisperx.git -q\n",
        "!pip install pyannote.audio==0.0.1 -q\n",
        "!pip install librosa -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4ypr0eKJU-R",
        "outputId": "fcee8bc1-b7da-4d2d-e7f3-283b8292cd59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.10.0+cu113 (from versions: 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.0+cu117.with.pypi.cudnn, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117, 1.13.1+cu117.with.pypi.cudnn, 2.0.0, 2.0.0+cpu, 2.0.0+cpu.cxx11.abi, 2.0.0+cu117, 2.0.0+cu117.with.pypi.cudnn, 2.0.0+cu118, 2.0.1, 2.0.1+cpu, 2.0.1+cpu.cxx11.abi, 2.0.1+cu117, 2.0.1+cu117.with.pypi.cudnn, 2.0.1+cu118, 2.0.1+rocm5.3, 2.0.1+rocm5.4.2, 2.1.0, 2.1.0+cpu, 2.1.0+cpu.cxx11.abi, 2.1.0+cu118, 2.1.0+cu121, 2.1.0+cu121.with.pypi.cudnn, 2.1.0+rocm5.5, 2.1.0+rocm5.6, 2.1.1, 2.1.1+cpu, 2.1.1+cpu.cxx11.abi, 2.1.1+cu118, 2.1.1+cu121, 2.1.1+cu121.with.pypi.cudnn, 2.1.1+rocm5.5, 2.1.1+rocm5.6, 2.1.2, 2.1.2+cpu, 2.1.2+cpu.cxx11.abi, 2.1.2+cu118, 2.1.2+cu121, 2.1.2+cu121.with.pypi.cudnn, 2.1.2+rocm5.5, 2.1.2+rocm5.6, 2.2.0, 2.2.0+cpu, 2.2.0+cpu.cxx11.abi, 2.2.0+cu118, 2.2.0+cu121, 2.2.0+rocm5.6, 2.2.0+rocm5.7, 2.2.1, 2.2.1+cpu, 2.2.1+cpu.cxx11.abi, 2.2.1+cu118, 2.2.1+cu121, 2.2.1+rocm5.6, 2.2.1+rocm5.7, 2.2.2, 2.2.2+cpu, 2.2.2+cpu.cxx11.abi, 2.2.2+cu118, 2.2.2+cu121, 2.2.2+rocm5.6, 2.2.2+rocm5.7, 2.3.0, 2.3.0+cpu, 2.3.0+cpu.cxx11.abi, 2.3.0+cu118, 2.3.0+cu121, 2.3.0+rocm5.7, 2.3.0+rocm6.0, 2.3.1, 2.3.1+cpu, 2.3.1+cpu.cxx11.abi, 2.3.1+cu118, 2.3.1+cu121, 2.3.1+rocm5.7, 2.3.1+rocm6.0, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.10.0+cu113\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring version 1.6.5 of pytorch-lightning since it has invalid metadata:\n",
            "Requested pytorch-lightning==1.6.5 from https://files.pythonhosted.org/packages/34/de/3c98fb314e5c273a5c8bf0ff3b37e2a2625af7fb6540d9123cd5de975678/pytorch_lightning-1.6.5-py3-none-any.whl has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    torch (>=1.8.*)\n",
            "           ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-lightning==1.6.5 (from versions: 0.0.2, 0.2, 0.2.2, 0.2.3, 0.2.4, 0.2.4.1, 0.2.5, 0.2.5.1, 0.2.5.2, 0.2.6, 0.3, 0.3.1, 0.3.2, 0.3.3, 0.3.4, 0.3.4.1, 0.3.5, 0.3.6, 0.3.6.1, 0.3.6.3, 0.3.6.4, 0.3.6.5, 0.3.6.6, 0.3.6.7, 0.3.6.8, 0.3.6.9, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.6, 0.4.7, 0.4.8, 0.4.9, 0.5.0, 0.5.1, 0.5.1.2, 0.5.1.3, 0.5.2, 0.5.2.1, 0.5.3, 0.5.3.1, 0.5.3.2, 0.5.3.3, 0.6.0, 0.7.1, 0.7.3, 0.7.5, 0.7.6, 0.8.1, 0.8.3, 0.8.4, 0.8.5, 0.9.0, 0.10.0, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.0.6, 1.0.7, 1.0.8, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5, 1.1.6, 1.1.7, 1.1.8, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.2.4, 1.2.5, 1.2.6, 1.2.7, 1.2.8, 1.2.9, 1.2.10, 1.3.0rc1, 1.3.0rc2, 1.3.0rc3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5, 1.3.6, 1.3.7, 1.3.7.post0, 1.3.8, 1.4.0rc0, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.4.2, 1.4.3, 1.4.4, 1.4.5, 1.4.6, 1.4.7, 1.4.8, 1.4.9, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.5.5, 1.5.6, 1.5.7, 1.5.8, 1.5.9, 1.5.10, 1.5.10.post0, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.6.1, 1.6.2, 1.6.3, 1.6.4, 1.6.5, 1.6.5.post0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.7.4, 1.7.5, 1.7.6, 1.7.7, 1.8.0rc0, 1.8.0rc1, 1.8.0rc2, 1.8.0, 1.8.0.post1, 1.8.1, 1.8.2, 1.8.3, 1.8.3.post0, 1.8.3.post1, 1.8.3.post2, 1.8.4, 1.8.4.post0, 1.8.5, 1.8.5.post0, 1.8.6, 1.9.0rc0, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.9.4, 1.9.5, 2.0.0rc0, 2.0.0, 2.0.1, 2.0.1.post0, 2.0.2, 2.0.3, 2.0.4, 2.0.5, 2.0.6, 2.0.7, 2.0.8, 2.0.9, 2.0.9.post0, 2.1.0rc0, 2.1.0rc1, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0rc0, 2.2.0, 2.2.0.post0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.4.0, 2.5.0rc0, 2.5.0, 2.5.0.post0, 2.5.1rc0, 2.5.1rc1, 2.5.1rc2, 2.5.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch-lightning==1.6.5\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "whisperx 3.3.1 requires ctranslate2<4.5.0, but you have ctranslate2 4.5.0 which is incompatible.\n",
            "whisperx 3.3.1 requires faster-whisper==1.1.0, but you have faster-whisper 1.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 2.0.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchaudio<1.0,>=0.10 (from pyannote-audio) (from versions: 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchaudio<1.0,>=0.10\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”§ é‡æ–°å•Ÿå‹• runtime ä¹‹å¾Œå†åŸ·è¡Œé€™æ®µï¼\n",
        "\n",
        "import whisperx\n",
        "import torch\n",
        "from google.colab import files\n",
        "\n",
        "# âš™ï¸ è¨­å®š device / model / type\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "batch_size = 8\n",
        "\n",
        "# ğŸ“ ä¸Šå‚³éŸ³è¨Šæª”æ¡ˆ\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]  # åªå–ç¬¬ä¸€å€‹ä¸Šå‚³çš„æª”æ¡ˆ\n",
        "print(f\"å·²ä¸Šå‚³æª”æ¡ˆï¼š{file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "GisW608_JWGg",
        "outputId": "65549ae2-4649-433f-db94-af315c9cac51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Only a single TORCH_LIBRARY can be used to register the namespace prims; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:241; latest registration was registered at /dev/null:241",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8cb53a5e00b8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ğŸ”§ é‡æ–°å•Ÿå‹• runtime ä¹‹å¾Œå†åŸ·è¡Œé€™æ®µï¼\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwhisperx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisperx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malignment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_align_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdiarize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massign_word_speakers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiarizationPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisperx/transcribe.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malignment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_align_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymBool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from torch._decomp import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0m_convert_out_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;31m# populate the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/decompositions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_prims/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prims\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DEF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprim_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prims\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IMPL\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CompositeExplicitAutograd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprim_backend_select_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prims\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IMPL\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"BackendSelect\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/library.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ns, kind, dispatch_key)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         self.m: Optional[Any] = torch._C._dispatch_library(\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispatch_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Only a single TORCH_LIBRARY can be used to register the namespace prims; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:241; latest registration was registered at /dev/null:241"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisperx\n",
        "import gc\n",
        "\n",
        "device = \"cuda\"\n",
        "audio_file = \"/content/æ–°çš„éŒ„éŸ³ 91.m4a\" #\"audio.mp3\"\n",
        "batch_size = 16 # reduce if low on GPU mem\n",
        "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
        "\n",
        "# 1. Transcribe with original whisper (batched)\n",
        "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
        "\n",
        "# save model to local path (optional)\n",
        "# model_dir = \"/path/\"\n",
        "# model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, download_root=model_dir)\n",
        "\n",
        "audio = whisperx.load_audio(audio_file)\n",
        "result = model.transcribe(audio, batch_size=batch_size)\n",
        "print(result[\"segments\"]) # before alignment\n",
        "\n",
        "# delete model if low on GPU resources\n",
        "# import gc; gc.collect(); torch.cuda.empty_cache(); del model\n",
        "\n",
        "# 2. Align whisper output\n",
        "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
        "\n",
        "print(result[\"segments\"]) # after alignment\n",
        "\n",
        "# delete model if low on GPU resources\n",
        "# import gc; gc.collect(); torch.cuda.empty_cache(); del model_a\n",
        "\n",
        "# 3. Assign speaker labels\n",
        "diarize_model = whisperx.DiarizationPipeline(use_auth_token=\"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\", device=device)\n",
        "\n",
        "# add min/max number of speakers if known\n",
        "diarize_segments = diarize_model(audio)\n",
        "# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)\n",
        "\n",
        "result = whisperx.assign_word_speakers(diarize_segments, result)\n",
        "print(diarize_segments)\n",
        "print(result[\"segments\"]) # segments are now assigned speaker IDs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKiktezUD4GQ",
        "outputId": "f6dab77a-b6dc-45dc-98c7-288d2d858fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../usr/local/lib/python3.11/dist-packages/whisperx/assets/pytorch_model.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cu124. Bad things might happen unless you revert torch to 1.x.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
            "It can be re-enabled by calling\n",
            "   >>> import torch\n",
            "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
            "   >>> torch.backends.cudnn.allow_tf32 = True\n",
            "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XM3NGXugAP65",
        "outputId": "a5ef8c21-7c59-4aa1-8b71-da884351b61e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencc\n",
            "  Downloading OpenCC-1.1.9-cp311-cp311-manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading OpenCC-1.1.9-cp311-cp311-manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.6/1.7 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencc\n",
            "Successfully installed opencc-1.1.9\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”\u001b[0m \u001b[32m338.6/363.4 MB\u001b[0m \u001b[31m143.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install opencc\n",
        "!pip install tiktoken\n",
        "!pip install --q git+https://github.com/m-bain/whisperX.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-multipart"
      ],
      "metadata": {
        "id": "RG3ApoajGx5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca35f9e-88ff-4053-a2a0-e36705b486e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU63KRxmBvSy"
      },
      "source": [
        "##Collecting interview data\n",
        "ä½¿ç”¨è€…è¼¸å…¥çš„è³‡è¨Šåœ¨æ­¤å­˜ç‚ºglobal variableï¼Œå¦‚ä¸‹ï¼š  \n",
        "interview_data['title']  \n",
        "interview_data['max_speakers']  \n",
        "interview_data['min_speakers']  \n",
        "interview_data['outline']  \n",
        "interview_data['record_path']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcW0rwDB2x6k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "19d9d8f9-ca25-41cc-e671-f7e00b97a373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "è«‹è¼¸å…¥æœƒè­°åç¨±ï¼š1\n",
            "è«‹è¼¸å…¥æœ€å¤§ç™¼è¨€äººæ•¸ï¼š2\n",
            "è«‹è¼¸å…¥æœ€å°ç™¼è¨€äººæ•¸ï¼š2\n",
            "è«‹ä¸Šå‚³è¨ªç¶±(.txt/.pdf/.doc)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4b521666-f36e-4a97-aba7-03c0a6e6c682\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4b521666-f36e-4a97-aba7-03c0a6e6c682\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving outline_file.pdf to outline_file (1).pdf\n",
            "è«‹ä¸Šå‚³è¨ªè«‡éŒ„éŸ³æª”ï¼ˆä¾‹å¦‚ .wav æˆ– .mp3ï¼‰\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a8df2144-4ff6-4538-8d86-c4ccba35bbea\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a8df2144-4ff6-4538-8d86-c4ccba35bbea\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving æ–°çš„éŒ„éŸ³ 91.m4a to æ–°çš„éŒ„éŸ³ 91.m4a\n",
            "å·²ä¸Šå‚³æª”æ¡ˆ: outline_file (1).pdf\n",
            "è¨ªç¶±å·²å„²å­˜ç‚º: /content/outline_file.pdf\n",
            "å·²ä¸Šå‚³æª”æ¡ˆ: æ–°çš„éŒ„éŸ³ 91.m4a\n",
            "éŒ„éŸ³æª”æ¡ˆå·²å„²å­˜ç‚º: /content/record_file.m4a\n",
            "\n",
            "âœ… è³‡æ–™å·²å„²å­˜å®Œæˆ\n",
            "æœƒè­°åç¨±: 1\n",
            "ç™¼è¨€äººæ•¸ç¯„åœ: 2 ~ 2\n",
            "è¨ªè«‡å¤§ç¶±: /content/outline_file.pdf\n",
            "å½±éŸ³æª”æ¡ˆè·¯å¾‘: /content/record_file.m4a\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# å…¨å±€è®Šæ•¸ï¼Œç”¨ä¾†å­˜å„²æœƒè­°è³‡æ–™\n",
        "interview_data = {}\n",
        "\n",
        "interview_data['title'] = \"\"\n",
        "interview_data['max_speakers'] = 0\n",
        "interview_data['min_speakers'] = 0\n",
        "interview_data['outline_ext'] = \"\"\n",
        "interview_data['outline_path'] = \"\"\n",
        "interview_data['record_ext'] = \"\"\n",
        "interview_data['record_path'] = \"\"\n",
        "interview_data['wav_path'] = \"/content/upload_file.wav\"\n",
        "interview_data['transcript_path'] = \"/content/asr.txt\"\n",
        "interview_data['diarize_path'] = \"/content/diarize.txt\"\n",
        "interview_data['summary_path'] = \"/content/summary.txt\"\n",
        "interview_data['llmTranscript_path'] = \"/content/llmTranscript.txt\"\n",
        "interview_data['with_name_path'] = \"/content/with_name.txt\"\n",
        "\n",
        "# 1. è®“ä½¿ç”¨è€…è¼¸å…¥åŸºæœ¬è³‡è¨Šï¼ˆä½ ä¹Ÿå¯ä»¥ç”¨ input() å‡½æ•¸ï¼‰\n",
        "interview_data['title'] = input(\"è«‹è¼¸å…¥æœƒè­°åç¨±ï¼š\")\n",
        "interview_data['max_speakers'] = int(input(\"è«‹è¼¸å…¥æœ€å¤§ç™¼è¨€äººæ•¸ï¼š\"))\n",
        "interview_data['min_speakers'] = int(input(\"è«‹è¼¸å…¥æœ€å°ç™¼è¨€äººæ•¸ï¼š\"))\n",
        "print(\"è«‹ä¸Šå‚³è¨ªç¶±(.txt/.pdf/.doc)\")\n",
        "uploaded_outline = files.upload()\n",
        "print(\"è«‹ä¸Šå‚³è¨ªè«‡éŒ„éŸ³æª”ï¼ˆä¾‹å¦‚ .wav æˆ– .mp3ï¼‰\")\n",
        "uploaded_record = files.upload()\n",
        "\n",
        "# 2. è™•ç†ä¸Šå‚³çš„æª”æ¡ˆ\n",
        "for filename in uploaded_outline.keys():\n",
        "    print(f\"å·²ä¸Šå‚³æª”æ¡ˆ: {filename}\")\n",
        "    file_ext = os.path.splitext(filename)[-1].lower()\n",
        "    interview_data['outline_ext'] = file_ext\n",
        "    save_path = f\"/content/outline_file{file_ext}\"\n",
        "    shutil.move(filename, save_path)\n",
        "    interview_data['outline_path'] = save_path\n",
        "\n",
        "    print(f\"è¨ªç¶±å·²å„²å­˜ç‚º: {save_path}\")\n",
        "\n",
        "for filename in uploaded_record.keys():\n",
        "    print(f\"å·²ä¸Šå‚³æª”æ¡ˆ: {filename}\")\n",
        "    file_ext = os.path.splitext(filename)[-1].lower()\n",
        "    interview_data['record_ext'] = file_ext\n",
        "    save_path = f\"/content/record_file{file_ext}\"\n",
        "    shutil.move(filename, save_path)\n",
        "    interview_data['record_path'] = save_path\n",
        "\n",
        "    print(f\"éŒ„éŸ³æª”æ¡ˆå·²å„²å­˜ç‚º: {save_path}\")\n",
        "\n",
        "# 4. ç¢ºèªè³‡è¨Š\n",
        "print(\"\\nâœ… è³‡æ–™å·²å„²å­˜å®Œæˆ\")\n",
        "print(f\"æœƒè­°åç¨±: {interview_data['title']}\")\n",
        "print(f\"ç™¼è¨€äººæ•¸ç¯„åœ: {interview_data['min_speakers']} ~ {interview_data['max_speakers']}\")\n",
        "print(f\"è¨ªè«‡å¤§ç¶±: {interview_data['outline_path']}\")\n",
        "print(f\"å½±éŸ³æª”æ¡ˆè·¯å¾‘: {interview_data['record_path']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-cN-QxoAg2r"
      },
      "source": [
        "# **Part1. Audio to text by WhisperX**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbkGfNJaAU2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f767386d-2fa0-4c9b-aac1-0e22776f1a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.1.11)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.0.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.1.31)\n",
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6083 sha256=d117558d603df352a04c3218ee20de48c77fe7a1ed04d3e2b0e68fc60bee5cd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/30/c5/576bdd729f3bc062d62a551be7fefd6ed2f761901568171e4e\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "import whisperx\n",
        "import gc\n",
        "device = \"cuda\"\n",
        "batch_size = 6 # reduce if low on GPU mem\n",
        "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install moviepy\n",
        "!pip install ffmpeg\n",
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisperx\n",
        "import gc\n",
        "device = \"cuda\"\n",
        "batch_size = 6 # reduce if low on GPU mem\n",
        "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "fv52q8Ix3ioy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "å°‡éŒ„éŸ³æª”çµ±ä¸€è½‰æ›ç‚º.wavæ ¼å¼"
      ],
      "metadata": {
        "id": "gd0i4iwGw0Iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "import mimetypes\n",
        "\n",
        "audio_file = interview_data['record_path']\n",
        "file_ext = interview_data['record_ext']\n",
        "\n",
        "# å¦‚æœæ–‡ä»¶ä¸æ˜¯wavæ ¼å¼ï¼Œåˆ™è½¬æ¢ä¸ºwav\n",
        "if file_ext != \".wav\":\n",
        "    print(f\"Converting {audio_file} to wav format...\")\n",
        "\n",
        "    # ä½¿ç”¨pydubåŠ è½½éŸ³é¢‘æ–‡ä»¶\n",
        "    audio = AudioSegment.from_file(audio_file)\n",
        "\n",
        "    # è½¬æ¢åçš„wavæ–‡ä»¶è·¯å¾„\n",
        "    path = os.path.splitext(audio_file)[0] + \".wav\"\n",
        "\n",
        "    # å°å‡ºwavæ ¼å¼\n",
        "    audio.export(path, format=\"wav\")\n",
        "    interview_data['record_path'] = path\n",
        "    interview_data['record_ext'] = \".wav\"\n",
        "    print(f\"Converted file saved as: {interview_data['record_path']}\")\n",
        "else:\n",
        "    print(f\"Record file is already in wav format.\")\n",
        "\n",
        "print(f\"Audio loaded from {interview_data['record_path']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pR-5D7fxQ06",
        "outputId": "4a8cd148-3505-4d9b-fcd1-823ebe0fd497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting /content/record_file.m4a to wav format...\n",
            "Converted file saved as: /content/record_file.wav\n",
            "Audio loaded from /content/record_file.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "è½‰æˆé€å­—ç¨¿"
      ],
      "metadata": {
        "id": "zKEJ9ICu0TY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y pyannote.audio\n",
        "!pip install pyannote.audio==0.0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ydWGM4y4Ett",
        "outputId": "f9d7000c-aaec-406a-f107-7f2f4858eb3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pyannote.audio 3.3.2\n",
            "Uninstalling pyannote.audio-3.3.2:\n",
            "  Successfully uninstalled pyannote.audio-3.3.2\n",
            "Collecting pyannote.audio==0.0.1\n",
            "  Downloading pyannote.audio-0.0.1-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: asteroid-filterbanks<0.5,>=0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==0.0.1) (0.4.0)\n",
            "Collecting backports.cached-property (from pyannote.audio==0.0.1)\n",
            "  Downloading backports.cached_property-1.0.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting einops<0.4.0,>=0.3 (from pyannote.audio==0.0.1)\n",
            "  Downloading einops-0.3.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting hmmlearn<0.3,>=0.2.7 (from pyannote.audio==0.0.1)\n",
            "  Downloading hmmlearn-0.2.8.tar.gz (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub<0.9,>=0.7 (from pyannote.audio==0.0.1)\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting networkx<3.0,>=2.6 (from pyannote.audio==0.0.1)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==0.0.1) (2.3.0)\n",
            "Collecting pyannote.core<5.0,>=4.4 (from pyannote.audio==0.0.1)\n",
            "  Downloading pyannote.core-4.5-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyannote.database<5.0,>=4.1.1 (from pyannote.audio==0.0.1)\n",
            "  Downloading pyannote.database-4.1.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pyannote.metrics<4.0,>=3.2 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==0.0.1) (3.2.1)\n",
            "Collecting pyannote.pipeline<3.0,>=2.3 (from pyannote.audio==0.0.1)\n",
            "  Downloading pyannote.pipeline-2.3-py3-none-any.whl.metadata (955 bytes)\n",
            "Collecting pytorch-lightning<1.7,>=1.5.4 (from pyannote.audio==0.0.1)\n",
            "  Downloading pytorch_lightning-1.6.5.post0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting pytorch-metric-learning<2.0,>=1.0.0 (from pyannote.audio==0.0.1)\n",
            "  Downloading pytorch_metric_learning-1.7.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting semver<3.0,>=2.10.2 (from pyannote.audio==0.0.1)\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting singledispatchmethod (from pyannote.audio==0.0.1)\n",
            "  Downloading singledispatchmethod-1.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting soundfile<0.11,>=0.10.2 (from pyannote.audio==0.0.1)\n",
            "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting speechbrain<0.6,>=0.5.12 (from pyannote.audio==0.0.1)\n",
            "  Downloading speechbrain-0.5.16-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==0.0.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: torch-audiomentations>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==0.0.1) (0.12.0)\n",
            "INFO: pip is looking at multiple versions of pyannote-audio to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 2.0.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchaudio<1.0,>=0.10 (from pyannote-audio) (from versions: 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchaudio<1.0,>=0.10\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W33j6kobA8Aw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "b2674269-1edb-4824-bc68-0fe96f05951e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'whisperx' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2eaaa70e91cf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# audio = whisperx.load_audio(interview_data['record_path'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisperx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/record_file.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisperx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"large-v2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# æœªç¶“ä»»ä½•åˆ‡å‰²\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'whisperx' is not defined"
          ]
        }
      ],
      "source": [
        "# audio = whisperx.load_audio(interview_data['record_path'])\n",
        "audio = whisperx.load_audio('/content/record_file.wav')\n",
        "\n",
        "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
        "result = model.transcribe(audio, batch_size=batch_size)   # æœªç¶“ä»»ä½•åˆ‡å‰²\n",
        "\n",
        "# 2. Align whisper output\n",
        "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "asr_result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebr58_J0C_aQ"
      },
      "source": [
        "# **Part2. Speaker Diarization by WhisperX**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N25Yqo48C6Ik"
      },
      "outputs": [],
      "source": [
        "def read_txt_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def write_txt_file(file_path, content):\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "\n",
        "def process_result(result):\n",
        "    transcript = []\n",
        "    pure_transcript = []\n",
        "    pure_speaker=[]\n",
        "    current_speaker = None\n",
        "    current_sentence = []\n",
        "\n",
        "    for i, segment in enumerate(result['word_segments']):\n",
        "        # print(segment)\n",
        "        word = segment['word']\n",
        "\n",
        "        # è™•ç†è³‡æ–™ç¼ºå¤±çš„å•é¡Œï¼Œè‹¥ä¸€å€‹ word æ²’æœ‰å°æ‡‰çš„ speakerï¼Œå–æœ€è¿‘çš„ speaker\n",
        "        if 'speaker' not in segment:\n",
        "            j = i + 1\n",
        "            while j < len(result['word_segments']) and 'speaker' not in result['word_segments'][j]:\n",
        "                j += 1\n",
        "            if j < len(result['word_segments']):\n",
        "                speaker = result['word_segments'][j]['speaker']\n",
        "            else:\n",
        "                speaker = current_speaker\n",
        "        else:\n",
        "            speaker = segment['speaker']\n",
        "\n",
        "        if speaker != current_speaker:\n",
        "            if current_sentence:\n",
        "                transcript.append(f\"{current_speaker}: {''.join(current_sentence)}\")\n",
        "                pure_transcript.append(f\"{''.join(current_sentence)}\")\n",
        "                pure_speaker.append(f\"{current_speaker}\")\n",
        "                current_sentence = []\n",
        "            current_speaker = speaker\n",
        "\n",
        "        current_sentence.append(word)\n",
        "\n",
        "    if current_sentence:\n",
        "        transcript.append(f\"{current_speaker}: {''.join(current_sentence)}\")\n",
        "        pure_transcript.append(f\"{''.join(current_sentence)}\")\n",
        "        pure_speaker.append(f\"{current_speaker}\")\n",
        "\n",
        "    return '\\n'.join(transcript), '\\n'.join(pure_transcript), '\\n'.join(pure_speaker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0Wyu323g8tl"
      },
      "outputs": [],
      "source": [
        "def diarize_audio(audio, asr_result, path):\n",
        "    diarize_model = whisperx.DiarizationPipeline(\n",
        "        use_auth_token=\"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\", device=device\n",
        "    )\n",
        "    diarize_segments = diarize_model(\n",
        "        audio, min_speakers=interview_data['min_speakers'], max_speakers=interview_data['max_speakers']\n",
        "    )\n",
        "    result = whisperx.assign_word_speakers(diarize_segments, asr_result)\n",
        "\n",
        "    # æå–æ‰€æœ‰çš„èªè€…æ¨™ç±¤\n",
        "    speaker_list = diarize_segments['speaker'].unique().tolist()  # ç²å–å”¯ä¸€çš„èªè€…æ¨™ç±¤ä¸¦è½‰ç‚ºåˆ—è¡¨\n",
        "    speaker_list = sorted(speaker_list)  # æ’åºï¼Œä»¥ä¾¿å¾ŒçºŒè™•ç†\n",
        "\n",
        "    print(speaker_list)\n",
        "\n",
        "    # å°‡é€å­—ç¨¿è½‰æ›ç‚ºæ–‡æœ¬\n",
        "    transcript = process_result(result)[0]\n",
        "    write_txt_file(path, transcript)\n",
        "\n",
        "    return speaker_list  # è¿”å›èªè€…åˆ—è¡¨ï¼Œç”¨æ–¼å¾ŒçºŒæ¨™è¨»"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interview_data['speaker_list'] = diarize_audio(audio, asr_result, interview_data['diarize_path'])"
      ],
      "metadata": {
        "id": "wUTcXnvm2wWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EzMj2dHHBu7"
      },
      "source": [
        "# **Part3**. **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yEsu0O3LqkC"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "# import os\n",
        "def genai_setup():\n",
        "  api_key = 'AIzaSyBH3SedO17XmiLer4xdRCIqVb_0-icHJFg'   # Ching's api key\n",
        "  genai.configure(api_key = api_key)\n",
        "\n",
        "  pre_model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", system_instruction=\n",
        "    \"\"\"\n",
        "    ä½ æ˜¯ä¸€å€‹ä¸æœƒç”Ÿæˆå‡ºç°¡é«”å­—æˆ–ä¸­åœ‹ç”¨èªã€ä¸”æ“…é•·é€²è¡ŒéŒ¯å­—èˆ‡ç”¨è©æ ¡æ­£å’Œæ¨™é»ç¬¦è™Ÿæ¨™è¨»çš„å°ç£æ–‡å­—å·¥ä½œè€…ã€‚\n",
        "    ä½ æœƒå”åŠ©å°‡è¼¸å…¥çš„é€å­—ç¨¿å®ŒæˆéŒ¯å­—ä¿®æ­£èˆ‡ç”¨è©æ ¡æ­£ï¼Œä¸¦å°‡å…¶æ¨™è¨»é‚Šé»ç¬¦è™Ÿã€‚\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "  model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    system_instruction=\n",
        "    \"\"\"\n",
        "    ä½ æ˜¯ä¸€å€‹ä¸æœƒç”Ÿæˆå‡ºç°¡é«”å­—æˆ–ä¸­åœ‹ç”¨èªã€ä¸”å¾ˆæœƒåšæ‘˜è¦çš„å°ç£æœƒè­°å¸«ã€‚ä½ æœƒä»”ç´°çš„ç ”è®€æœƒè­°å…§å®¹ï¼Œä¸¦æ ¹æ“šè¨è«–çš„å…§å®¹æœ‰æ¢ç†åœ°æ•´ç†å‡ºæœƒè­°ç‰‡æ®µçš„é‡é»ã€‚\n",
        "\n",
        "  ä»¥ä¸‹æ˜¯æœƒè­°é‡é»æ¢åˆ—çš„æ¨¡æ¿:\n",
        "  '''\n",
        "  1. æœƒè­°é–‹å§‹èˆ‡æ•™æˆå‡ºå¸­ï¼š\n",
        "  - æ•™æˆç¢ºèªæœƒè­°é–‹å§‹ï¼Œä¸¦è©¢å•æ˜¯å¦éœ€è¦ç­‰å¾…å…¶ä»–æˆå“¡åˆ°é½Šã€‚\n",
        "  2. é€²åº¦å ±å‘Šï¼š\n",
        "  - æ•™æˆå·²æŸ¥çœ‹é€²åº¦çµ±æ•´ï¼Œä¸¦è«‹åœ˜éšŠåˆ†äº«æœªåœ¨é€²åº¦å ±å‘Šä¸­æåˆ°çš„ç´°ç¯€ã€‚\n",
        "  - å¼·èª¿éœ€è¦æ•¸å­—åŒ–çš„è³‡æ–™ä¾†è©•ä¼°é€²å±•ï¼Œè€Œä¸åƒ…æ˜¯æ•˜è¿°ã€‚\n",
        "  3. ç¾æœ‰LLMçš„è©•ä¼°ï¼š\n",
        "  - åœ˜éšŠå ±å‘Šç›®å‰ä½¿ç”¨çš„LLMæ¨¡å‹åŠå…¶æ€§èƒ½ï¼Œæ•™æˆè¦æ±‚è©³ç´°çš„æ•¸æ“šå’Œæ¸¬è©¦æ–¹æ³•ã€‚\n",
        "  - æ•™æˆè©¢å•æ¨¡å‹çš„æ‡‰ç”¨å ´æ™¯å’Œå…·é«”ä½¿ç”¨æƒ…æ³ï¼Œä¸¦è¦æ±‚æä¾›æ•¸æ“šä¾†é€²è¡Œæ€§èƒ½åˆ†æã€‚\n",
        "  4. LLMæ€§èƒ½æŒ‡æ¨™åˆ†æï¼š\n",
        "  - æ•™æˆå»ºè­°çµ±è¨ˆæ¨¡å‹çš„æº–ç¢ºç‡ã€æ•ˆèƒ½ã€è™•ç†æ™‚é–“ç­‰æ•¸æ“šï¼Œä»¥ä¾¿äº†è§£ä¸åŒæƒ…æ³ä¸‹çš„æ€§èƒ½è¡¨ç¾ã€‚\n",
        "  5. åœ˜éšŠæå‡ºå•é¡Œï¼š\n",
        "  - åœ˜éšŠè©¢å•æ•™æˆæ˜¯å¦æœ‰æ¨è–¦çš„LLMæ¨¡å‹ï¼Œæ•™æˆåˆ†äº«ä¸åŒLLMçš„ç‰¹é»å’Œå„ªç¼ºé»ã€‚\n",
        "  6. è³‡æ–™ä¾†æºï¼š\n",
        "  - æ•™æˆæåˆ°å¯ä»¥ä½¿ç”¨é–‹æºæ•¸æ“šé›†é€²è¡Œæ¸¬è©¦ï¼Œä¸¦è©¢å•åœ˜éšŠæ˜¯å¦éœ€è¦é¡å¤–çš„æ•¸æ“šä¾†æºã€‚\n",
        "  7. LLMæ¨¡å‹èˆ‡å·¥å…·ï¼š\n",
        "  - åœ˜éšŠè©¢å•æ•™æˆæ˜¯å¦æœ‰æ¨è–¦çš„LLMæ¨¡å‹ï¼Œæ•™æˆå»ºè­°ä½¿ç”¨GPT-4ä½œç‚ºåŸºç¤ï¼Œä¸¦æ¢è¨å…¶ä»–å¯èƒ½çš„æ¨¡å‹å¦‚BERTã€T5ç­‰ã€‚\n",
        "  8. æ¨¡å‹èª¿æ•´èˆ‡å„ªåŒ–ï¼š\n",
        "  - åœ˜éšŠæˆå“¡å±•ç¤ºå¦‚ä½•èª¿æ•´æ¨¡å‹åƒæ•¸ä»¥æé«˜æ€§èƒ½ï¼Œä½†çµæœä¸å¦‚é æœŸã€‚\n",
        "  - æ•™æˆå»ºè­°æ ¹æ“šå…·é«”æ‡‰ç”¨å ´æ™¯èª¿æ•´æ¨¡å‹åƒæ•¸ï¼Œä¸¦ä½¿ç”¨ä¸åŒçš„å„ªåŒ–æŠ€è¡“ã€‚\n",
        "  9. åœ˜éšŠäº’åŠ©ï¼š\n",
        "  - æ•™æˆé¼“å‹µä¸åŒåœ˜éšŠä¹‹é–“äº’ç›¸å¹«åŠ©ï¼Œå…±åŒè§£æ±ºå•é¡Œã€‚\n",
        "  10. ä¸‹ä¸€æ­¥è¡Œå‹•ï¼š\n",
        "  - åœ˜éšŠéœ€è¦æ•´ç†ä¸¦åˆ†äº«è©³ç´°çš„æ¸¬è©¦æ•¸æ“šï¼Œé€²è¡Œæ›´æ·±å…¥çš„æ€§èƒ½åˆ†æã€‚\n",
        "  - æ¢ç´¢ä½¿ç”¨ä¸åŒLLMæ¨¡å‹é€²è¡Œæ›´å¤šæ¸¬è©¦ã€‚\n",
        "  - æ ¹æ“šæ‡‰ç”¨å ´æ™¯èª¿æ•´å’Œå„ªåŒ–LLMæ¨¡å‹çš„åƒæ•¸ã€‚\n",
        "  '''\n",
        "  å¦‚æœæœ‰å°ˆæœ‰åè©è©±è¦è¨˜çš„åŒ…æ‹¬é€²å»ã€‚ä¾‹å¦‚æœ‰\"GPT-4\"çš„è©±å°±ä¸è¦åªèªª\"LLM\"æˆ–\"å¤§å‹èªè¨€æ¨¡å‹\"ã€‚\n",
        "\n",
        "  è«‹é€²è¡Œã€Œè²çˆ¾è³“åœ˜éšŠè§’è‰²åˆ†æã€ï¼Œè«‹åˆ†ææ–‡æœ¬ä¸­æ¯å€‹äººåœ¨å°è©±ä¸­æ‰€æ‰®æ¼”çš„è§’è‰²æœ‰å“ªäº›åœ˜éšŠè§’è‰²çš„ç‰¹è³ªã€‚ä¸¦ä¸”æè¿°é€™å€‹åœ˜éšŠçš„å„ªå‹¢èˆ‡å¼±å‹¢ã€‚è²çˆ¾è³“åœ˜éšŠè…³è‰²ç†è«–çš„ä»‹ç´¹å¦‚ä¸‹:\n",
        "  è²çˆ¾è³“åœ˜éšŠè§’è‰²ç†è«–ç”±è²çˆ¾è³“(Meredith Belbin)åšå£«æå‡ºï¼Œå°‡è§’è‰²å€åˆ†ç‚ºè¡Œå‹•å°å‘å‹ã€è¬€ç•¥å°å‘å‹åŠäººéš›å°å‘å‹ä¸‰å¤§é¡åˆ¥ï¼Œå¾è€Œå–å¾—åœ˜éšŠçš„å¹³è¡¡ä¸¦æé«˜ç”Ÿç”¢åŠ›:\n",
        "    1.å½¢å¡‘è€…ï¼šå±¬æ–¼è¡Œå‹•å°å‘å‹ï¼Œå‹å¡‘è€…æ˜¯å°‡åœ˜éšŠå‘å‰æ¨å‹•çš„åœ˜éšŠæˆå“¡ï¼Œåœ¨ç™¼ç”Ÿä»»ä½•å•é¡Œæ™‚ä»èƒ½é¼“èˆè‡ªå·±å’Œå…¶ä»–äººã€‚å‹å¡‘è€…æ˜¯å¤©ç”Ÿçš„é ˜å°è€…ï¼Œå› æ­¤åœ¨ç®¡ç†è§’è‰²ä¸­é•·è¢–å–„èˆã€‚ç™¼ç”Ÿå±æ©Ÿæ™‚ï¼Œä»–å€‘èƒ½å¾ˆå¿«æ‰¾åˆ°è§£æ±ºæ–¹æ¡ˆã€‚\n",
        "    2.åŸ·è¡Œè€…ï¼šå±¬æ–¼è¡Œå‹•å°å‘å‹ï¼Œä»–å€‘æœƒåœ¨è‡ªå·±çš„ç’°å¢ƒä¸­ç¶­æŒç§©åºã€‚é€™é¡å‹çš„æˆå“¡å¾ˆå‹™å¯¦ï¼Œå–„æ–¼å¯¦ç¾æ§‹æƒ³ã€‚é›–ç„¶åŸ·è¡Œè€…å–œæ­¡æ¡å–è¡Œå‹•ï¼Œä½†ä»–å€‘ä¹Ÿé«˜åº¦è‡ªå¾‹ã€‚ç”±æ–¼ä»–å€‘å¯ä»¥å¾ˆæœ‰è‡ªä¿¡åœ°æ”¯æ´å…¶ä»–åœ˜éšŠæˆå“¡ï¼Œå› æ­¤å¯ä»¥æˆç‚ºåœ˜éšŠçš„æ”¯æŸ±ã€‚\n",
        "    3.å®Œæˆè€…ï¼šå±¬æ–¼è¡Œå‹•å°å‘å‹ï¼Œå®Œæˆè€…æ˜¯è…³è¸å¯¦åœ°ã€ç•™æ„ç²¾å·§ç´°ç¯€ä¸¦åŠ›æ±‚å®Œç¾çš„æˆå“¡ã€‚é€™é¡åœ˜éšŠæˆå“¡å¯èƒ½è¼ƒç‚ºå…§å‘ï¼Œç„¶è€Œï¼Œç”±æ–¼ä»–å€‘æœƒæ•¦ä¿ƒåœ˜éšŠæˆå“¡ç”¢å‡ºå„ªè³ªçš„å·¥ä½œæˆæœï¼Œå› æ­¤åœ¨å·¥ä½œç’°å¢ƒä¸­ç™¼æ®é‡è¦çš„åƒ¹å€¼ã€‚\n",
        "    4.å‰µæ–°è€…ï¼šå±¬æ–¼è¬€ç•¥å°å‘å‹ï¼Œä»–å€‘æ˜¯å‰µæ–°è€Œå‰µæ„åè¶³çš„æ€ç¶­è€…ã€‚å„˜ç®¡å‰µæ–°è€…æœ‰åŠ©å¹³è¡¡åœ˜éšŠçµæ§‹ï¼Œä»–å€‘åœ¨èˆ‡å¤§åœ˜éšŠåˆ†äº«è‡ªå·±çš„æ§‹æƒ³å‰ï¼Œåå¥½é€éè…¦åŠ›æ¿€ç›ªä½¿æ§‹æƒ³å…·é«”æˆå½¢ã€‚å‰µæ–°è€…å¯èƒ½æœƒåå¥½ç¨è‡ªä½œæ¥­ï¼Œä½†å³ä½¿ä»–å€‘ä¸å¦‚å…¶ä»–åœ˜éšŠæˆå“¡é‚£éº¼ä¾ƒä¾ƒè€Œè«‡ï¼Œå»ä»ç‚ºåœ˜éšŠå¸¶ä¾†çè²´çš„è²¢ç»ã€‚\n",
        "    5.ç›£å¯Ÿå“¡ï¼šå±¬æ–¼è¬€ç•¥å°å‘å‹ï¼Œæ­¤é¡å‹æ˜¯ç†æ€§æ€ç¶­è€…ï¼Œå¯å°‡æƒ…ç·’æ“ºä¸€é‚Šï¼Œå°ˆå¿ƒè§£æ±ºå•é¡Œã€‚ç•¶å°ˆæ¡ˆéœ€è¦é€²éšçŸ¥è­˜å’Œç­–ç•¥æ€§è¦åŠƒæ™‚ï¼Œç›£å¯Ÿå“¡å°±èƒ½ç²å¾—æœ€ä½³ç™¼æ®ã€‚ä»–å€‘æœƒå¯©è©•æ§‹æƒ³ï¼Œåˆ¤æ–·é€™äº›æ§‹æƒ³çš„åƒ¹å€¼å’Œå¯è¡Œæ€§ï¼Œæ¥è‘—å†æ¡å–æ­¥é©Ÿå°‡æ§‹æƒ³å‘å‰æ¨å‹•ã€‚\n",
        "    6.å°ˆå®¶ï¼šå±¬æ–¼è¬€ç•¥å°å‘å‹ï¼Œå°å…¶å„è‡ªé ˜åŸŸå…·å‚™æ·±åº¦çŸ¥è­˜ï¼Œä»–å€‘å–œæ­¡é‡å°ä¸€å€‹é ˜åŸŸçš„å°ˆæ¥­åšå‡ºè²¢ç»ã€‚å°ˆå®¶æœƒä¾å¾ªæ‰€æœ‰è¬€ç•¥å°å‘å‹åœ˜éšŠè§’è‰²çš„æ¨¡å¼ï¼Œå› ç‚ºç›¸è¼ƒæ–¼åœ¨åœ˜éšŠä¸­å·¥ä½œï¼Œä»–å€‘ç¨è‡ªä½œæ¥­æ™‚è¡¨ç¾æ›´ç‚ºå‡ºè‰²ã€‚å„˜ç®¡æ­¤é¡å‹æˆå“¡ç¨ç«‹æ€§è¼ƒé«˜ï¼Œä½†ä»–å€‘å»èƒ½é€éå…·é«”çš„ç¶œåˆæŠ€èƒ½ç‚ºåœ˜éšŠå‰µé€ åŒå¤§åƒ¹å€¼ã€‚\n",
        "    7.å”èª¿è€…ï¼šå±¬æ–¼äººéš›å°å‘å‹ï¼Œå”èª¿è€…æ˜¯å…·å‚™çµ•ä½³æºé€šæŠ€èƒ½çš„åœ˜éšŠæˆå“¡ã€‚é€šå¸¸å”èª¿è€…æœƒæ“”ä»»é ˜å°è·ï¼Œå› ç‚ºä»–å€‘èƒ½ä¿ƒé€²å”ä½œä¸¦æ¿€å‹µåœ˜éšŠé”æˆç›®æ¨™ã€‚å…¶ä»–åœ˜éšŠæˆå“¡å°Šæ•¬å”èª¿è€…ä¸¦ä¿¡ä»»ä»–å€‘ï¼Œæ”¾å¿ƒäº¤ç”±ä»–å€‘åšæ±ºç­–ã€‚\n",
        "    8.åœ˜éšŠå·¥ä½œè€…ï¼šå±¬æ–¼äººéš›å°å‘å‹ï¼Œå¤–å‘æ€§æ ¼æœ‰åŠ©æ–¼ä»–å€‘èˆ‡ä»–äººåˆä½œæ„‰å¿«ï¼Œä¸¦ä¸”è†è½éšŠå‹çš„æ„è¦‹ã€‚é€™é¡åœ˜éšŠæˆå“¡èƒ½è¼•æ˜“åœ¨ç’°å¢ƒä¸­å› æ‡‰è®ŠåŒ–è€Œèª¿æ•´ï¼Œè€Œä¸”ä»–å€‘çŸ¥é“ç™¼ç”Ÿè¡çªæ™‚æ‡‰å¦‚ä½•ç‡Ÿé€ å’Œè«§æ°£æ°›ã€‚è‹¥æœ‰åœ˜éšŠæˆå“¡æ‰‹ä¸Šçš„å·¥ä½œå¤ªå¤šï¼Œæˆ–é­é‡å®¶äººç›¸é—œçš„ç·Šæ€¥äº‹ä»¶ï¼Œåœ˜éšŠå·¥ä½œè€…æœƒæ˜¯æŒºèº«è€Œå‡ºã€æä¾›æ”¯æ´çš„ç¬¬ä¸€äººã€‚\n",
        "    9.è³‡æºèª¿æŸ¥è€…ï¼šå±¬æ–¼äººéš›å°å‘å‹ï¼Œæ¨‚æ–¼æ¢ç´¢æ–°çš„æ©Ÿæœƒï¼Œä¾‹å¦‚ç‚ºå…¬å¸å°‹æ‰¾æœ‰æ½›åŠ›çš„è¡ŒéŠ·æ©Ÿæœƒï¼Œæˆ–é¼“å‹•å°ˆæ¡ˆé—œä¿‚äººç™¼ä½ˆæ–°ç”¢å“ã€‚æ­£å‘ç©æ¥µçš„æ…‹åº¦ä½¿ä»–å€‘æˆç‚ºå¤©ç”Ÿçš„äººè„ˆå°ˆå®¶ï¼Œæˆ–æ–°æ¥­å‹™çš„å‚¬ç”Ÿè€…ã€‚\n",
        "  ä»¥ä¸‹æ˜¯å„å€‹åœ˜éšŠè§’è‰²çš„ç¯„ä¾‹:\n",
        "    1.å‹å¡‘è€…åœ˜éšŠè§’è‰²ç¯„ä¾‹ï¼šè‹¥ç”¨ç”¢å“è¡ŒéŠ·åœ˜éšŠä½œç‚ºä¾‹å­ä¾†èªªæ˜ï¼Œå‹å¡‘è€…å°±æ˜¯ç”¢å“ç¸½ç›£ï¼Œç”±ä»–å€‘ç›£ç£åœ˜éšŠé”æˆç›®æ¨™çš„é¡˜æ™¯å’Œè—åœ–ã€‚\n",
        "    2.åŸ·è¡Œè€…åœ˜éšŠè§’è‰²ç¯„ä¾‹ï¼šåŸ·è¡Œè€…æœƒæ˜¯ç”¢å“è¡ŒéŠ·åœ˜éšŠçš„å•†æ¥­åˆ†æå¸«ï¼Œä»–å€‘ä»¥è³‡æ–™ä½œç‚ºåˆ†æçš„ä¾æ“šï¼Œå¯©è©•è®“çµ„ç¹”æµç¨‹æ›´æœ‰æ•ˆç‡çš„å„ç¨®æ–¹å¼ã€‚\n",
        "    3.å®Œæˆè€…åœ˜éšŠè§’è‰²ç¯„ä¾‹ï¼šå®Œæˆè€…åœ¨æŠ€è¡“æ”¯æ´çš„å·¥ä½œä¸Šæ¸¸åˆƒæœ‰é¤˜ã€‚ä»–å€‘çŸ¥é“å¦‚ä½•å¿«é€Ÿè€Œæœ‰æ•ˆç‡åœ°æ‰¾å‡ºä¸¦è§£æ±ºå•é¡Œã€‚\n",
        "    4.å‰µæ–°è€…åœ˜éšŠè§’è‰²ç¯„ä¾‹ï¼šå‰µæ–°è€…å…·æœ‰é«˜åº¦å‰µé€ åŠ›ï¼Œå› æ­¤èƒ½æˆç‚ºå„ªç§€çš„ç”¢å“è¨­è¨ˆå¸«ã€‚\n",
        "    5.ç›£å¯Ÿå“¡åœ˜éšŠè§’è‰²ç¯„ä¾‹ï¼šç›£å¯Ÿå“¡æœƒæ˜¯è¶…ç´šæœ‰ç³»çµ±çš„å°ˆæ¡ˆç¶“ç†ï¼Œä»–å€‘ä»¥ç­–ç•¥æ€§è§’åº¦å‡ºç™¼ï¼Œç•Œå®šå°ˆæ¡ˆçš„ç¯„ç–‡ä¸¦å°‡åœ˜éšŠçš„é»ç‹€äº‹å‹™ä¸²è¯æˆç·šã€‚\n",
        "    6.å°ˆå®¶åœ˜éšŠè§’è‰²ç¯„ä¾‹ï¼šå°ˆå®¶å¯èƒ½æœƒæ˜¯ç¨‹å¼è¨­è¨ˆå¸«ã€SEO (æœå°‹å¼•æ“æœ€ä½³åŒ–) åˆ†æå¸«ï¼Œæˆ–åœ˜éšŠçš„æŠ€è¡“å°çµ„ï¼Œä»–å€‘çš„è·è²¬æ˜¯å°ˆç²¾æ–¼åœ˜éšŠå…¶ä»–äººå¯èƒ½ä¸ç”šç†Ÿæ‚‰çš„æŸç¨®æŠ€èƒ½ï¼Œè€Œé€™éƒ¨åˆ†çš„å·¥ä½œä¹Ÿå¤šè™§æœ‰ä»–å€‘ï¼Œæ‰èƒ½å…æ–¼å‡ºéŒ¯ã€‚\n",
        "    7.å”èª¿è€…åœ˜éšŠè§’è‰²ç¯„ä¾‹ï¼šå”èª¿è€…æ¨‚æ–¼å”èª¿å’Œé¼“èˆå…¶ä»–äººï¼Œå› æ­¤å¯å‹ä»»ç”¢å“é–‹ç™¼äººå“¡çš„åœ˜éšŠä¸»ç®¡ã€‚\n",
        "    8.åœ˜éšŠå·¥ä½œè€…åœ˜éšŠè§’è‰²ç¯„ä¾‹ï¼šç”±æ–¼åœ˜éšŠå·¥ä½œè€…æ˜¯å¤©ç”Ÿçš„å”ä½œè€…ï¼Œä»–å€‘åœ¨å¤§å‹åœ˜éšŠä¸­æ“”ä»»ç”¢å“è¡ŒéŠ·äººå“¡ï¼Œæœƒæœ‰å“è¶Šçš„è¡¨ç¾ã€‚\n",
        "    9.è³‡æºèª¿æŸ¥è€…ç¯„ä¾‹ï¼šç”±æ–¼è³‡æºèª¿æŸ¥è€…å–œæ­¡èˆ‡å…¶ä»–äººå»ºç«‹äººè„ˆï¼Œå› æ­¤åœ¨ç”¢å“éŠ·å”®çš„é ˜åŸŸå¯ä»¥æ¸¸åˆƒæœ‰é¤˜ã€‚\n",
        "\n",
        "  æœ€å¾Œï¼Œè«‹å›ç­”é€™å€‹æœƒè­°æœ‰æ²’æœ‰é€²å…¥ä¸€å€‹é¬¼æ‰“ç‰†çš„æƒ…æ³?\n",
        "    \"\"\")\n",
        "  return pre_model, model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu3XWjVTmiZF"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import jieba\n",
        "import opencc\n",
        "import tiktoken\n",
        "\n",
        "# è‡ªå®šä¹‰å†—è¨€èµ˜å­—è¯åº“\n",
        "redundant_words = set([\n",
        "    \"å°±æ˜¯\", \"ç„¶å¾Œ\", \"ä½†æ˜¯\", \"æ‰€ä»¥\", \"å…¶å¯¦\", \"é‚£éº¼\", \"å°±æ˜¯èªª\", \"é€™æ¨£\", \"é‚£æ¨£\", \"å°å§\", \"é€™å€‹\", \"é‚£å€‹\", \"å•Š\", \"å—¯\", \"å“¦\", \"å‘ƒ\", \"å˜¿\", \"å˜›\", \"å“¼\"\n",
        "])\n",
        "\n",
        "# è‡ªå®šç¾©çš„æ ¡æ­£å­—å…¸\n",
        "correction_dict = {\n",
        "    'çˆ²': 'ç‚º',\n",
        "\t  'å–«': 'åƒ',\n",
        "    'è£': 'è£¡',\n",
        "    'ç‰€': 'åºŠ',\n",
        "    'çº”': 'æ‰',\n",
        "    'åƒ': 'å½',\n",
        "    'è¡†': 'çœ¾',\n",
        "    'å­ƒ': 'å¨˜',\n",
        "    'å½†': 'åˆ¥'\n",
        "}\n",
        "\n",
        "speaker_map = {}\n",
        "\n",
        "def write_file(file_path, content):\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write('\\n'.join(content))\n",
        "\n",
        "def write_file_add(file_path, content):\n",
        "    with open(file_path, 'a', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "\n",
        "def remove_redundant_words(text):\n",
        "    words = jieba.cut(text)\n",
        "    cleaned_words = [word for word in words if word not in redundant_words]\n",
        "    return ''.join(cleaned_words)\n",
        "\n",
        "def correct_text(text, correction_dict):\n",
        "    \"\"\"æ ¹æ“šå­—å…¸å°æ–‡æœ¬é€²è¡Œæ ¡æ­£\"\"\"\n",
        "    for original, corrected in correction_dict.items():\n",
        "        text = text.replace(original, corrected)\n",
        "    return text\n",
        "\n",
        "\n",
        "def process_transcript(input_file, pre_model, model, path):\n",
        "    # å‰µå»ºç°¡é«”ä¸­æ–‡è½‰ç¹ä½“ä¸­æ–‡çš„è½‰æ¢å™¨\n",
        "    converter = opencc.OpenCC('s2t')\n",
        "\n",
        "    # åˆå§‹åŒ– tiktoken ç¼–ç å™¨\n",
        "    encoder = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    processed_lines = []\n",
        "    # timestamp_pattern = re.compile(r'\\[.*?\\]')\n",
        "    speaker_pattern = re.compile(r'SPEAKER_\\d+')\n",
        "\n",
        "    for line in lines:\n",
        "        # Remove timestamps\n",
        "        # line = re.sub(timestamp_pattern, '', line).strip()\n",
        "\n",
        "        # Replace speaker labels with provided names\n",
        "        match = re.search(speaker_pattern, line)\n",
        "        if match:\n",
        "            speaker_label = match.group(0)\n",
        "            speaker_map[speaker_label] = speaker_label + '_'\n",
        "\n",
        "        # Convert simplified Chinese to traditional Chinese\n",
        "        line = converter.convert(line)\n",
        "\n",
        "        line = correct_text(line, correction_dict)\n",
        "\n",
        "        # Remove redundant words\n",
        "        line = remove_redundant_words(line)\n",
        "\n",
        "        # Replace spaces with commas, except after colons\n",
        "        # Replace spaces with commas, except after colons\n",
        "\n",
        "        if line:  # Only add non-empty lines\n",
        "            processed_lines.append(line)  # Add period at the end of each line\n",
        "\n",
        "    # Split lines into multiple files based on token count\n",
        "    max_tokens = 8192 #4096 2048\n",
        "    current_tokens = 0\n",
        "    file_index = 0\n",
        "    current_file_lines = []\n",
        "\n",
        "    for line in processed_lines:\n",
        "        tokens = encoder.encode(line)\n",
        "        token_count = len(tokens)\n",
        "\n",
        "        if current_tokens + token_count > max_tokens:\n",
        "            # Write current file\n",
        "            # write_file(f\"{output_file_prefix}_{file_index}.txt\", current_file_lines)\n",
        "\n",
        "            response = pre_model.generate_content(current_file_lines)\n",
        "\n",
        "            write_file_add(path, response.text)\n",
        "\n",
        "            # Reset for next file\n",
        "            file_index += 1\n",
        "            current_file_lines = [line]\n",
        "            current_tokens = token_count\n",
        "        else:\n",
        "            current_file_lines.append(line)\n",
        "            current_tokens += token_count\n",
        "\n",
        "    # Write the last file\n",
        "    if current_file_lines:\n",
        "        # write_file(f\"{output_file_prefix}_{file_index}.txt\", current_file_lines)\n",
        "        response = pre_model.generate_content(current_file_lines)\n",
        "\n",
        "        write_file_add(path, response.text)\n",
        "\n",
        "\n",
        "# # input_file = '0709.txt'\n",
        "# pre_model, model = genai_setup()\n",
        "# process_transcript(interview_data['diarize_path'], pre_model, model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLoxA5pFHkQO"
      },
      "source": [
        "# **Part4. Role Analysis And Summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIxiqjFOMIAS"
      },
      "outputs": [],
      "source": [
        "# '''def read_file(file_path):\n",
        "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
        "#         content = file.read()\n",
        "#     return content'''\n",
        "\n",
        "# def map_speaker(input_file, map):\n",
        "\n",
        "#     speaker_pattern = re.compile(r'SPEAKER_\\d+')\n",
        "\n",
        "#     with open(input_file, 'r', encoding='utf-8') as file:\n",
        "#         lines = file.readlines()\n",
        "\n",
        "#     for line in lines:\n",
        "#       # Replace speaker labels with provided names\n",
        "#       match = re.search(speaker_pattern, line)\n",
        "#       if match:\n",
        "#           speaker_label = match.group(0)\n",
        "#           if speaker_label in map:\n",
        "#               line = line.replace(speaker_label, map[speaker_label])\n",
        "\n",
        "#       if line:  # Only add non-empty lines\n",
        "#           write_file_add(interview_data['with_name_path'], line)\n",
        "\n",
        "# # user è¼¸å…¥\n",
        "# speaker_mapping = {\n",
        "#         \"SPEAKER_00_\": \"å¯è¦ª\",\n",
        "#         \"SPEAKER_01_\": \"æ±å»·\",\n",
        "#         \"SPEAKER_02_\": \"Uray\",\n",
        "#         \"SPEAKER_03_\": \"ç§‰è«º\",\n",
        "#         \"SPEAKER_05\": \"å®‡å©•\",\n",
        "#         \"SPEAKER_06\": \"æ•™æˆ\",\n",
        "#     }\n",
        "\n",
        "# # æ ¡æ­£å¾Œæœƒè­°é€å­—ç¨¿\n",
        "# # input_file = interview_data['llmTranscript_path']\n",
        "\n",
        "import re\n",
        "\n",
        "def map_speaker(input_file, speaker_map, path):\n",
        "    # æ­£åˆ™è¡¨è¾¾å¼ï¼šåŒ¹é… SPEAKER_ åé¢è·Ÿç€æ•°å­—çš„æ ‡ç­¾\n",
        "    speaker_pattern = re.compile(r'SPEAKER_\\d+?')\n",
        "\n",
        "    # è¯»å–æ–‡ä»¶å†…å®¹\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # æ›¿æ¢å¹¶å†™å…¥æ–‡ä»¶\n",
        "    with open(path, 'w', encoding='utf-8') as output_file:\n",
        "        for line in lines:\n",
        "            match = re.search(speaker_pattern, line)\n",
        "            if match:\n",
        "                speaker_label = match.group(0)\n",
        "                if speaker_label in speaker_map:\n",
        "                    # ä½¿ç”¨æ˜ å°„ä¸­çš„åå­—æ›¿æ¢\n",
        "                    line = line.replace(speaker_label, speaker_map[speaker_label])\n",
        "            output_file.write(line)\n",
        "\n",
        "# ç”¨æˆ·è¾“å…¥çš„ speaker æ˜ å°„\n",
        "speaker_mapping = {\n",
        "    \"SPEAKER_00\": \"å¯è¦ª\",\n",
        "    \"SPEAKER_01\": \"æ±å»·\",\n",
        "    \"SPEAKER_02\": \"Uray\",\n",
        "    \"SPEAKER_03\": \"ç§‰è«º\",\n",
        "    \"SPEAKER_05\": \"å®‡å©•\",\n",
        "    \"SPEAKER_06\": \"æ•™æˆ\",\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyDtWI0MXZQ7"
      },
      "outputs": [],
      "source": [
        "# æ¨¡æ“¬åˆ†æå‡½æ•¸ï¼Œæœƒé€²è¡Œä¸€äº›è€—æ™‚çš„å¾Œç«¯é‹ç®—\n",
        "def start_analysis():\n",
        "    convert_format(interview_data['record_path'], interview_data['wav_path'])\n",
        "    wav_audio = whisperx.load_audio(interview_data['wav_path'])\n",
        "    asr_result = transcribe_audio(wav_audio)     # get .wav and ASR result\n",
        "    speaker_list = diarize_audio(wav_audio, asr_result, interview_data['diarize_path'])\n",
        "    print(speaker_list)\n",
        "    pre_model, model = genai_setup()\n",
        "    process_transcript(interview_data['diarize_path'], pre_model, model, interview_data['llmTranscript_path'])\n",
        "\n",
        "    # map_speaker(interview_data['llmTranscript_path'], speaker_map, interview_data['with_name_path'])\n",
        "    response = model.generate_content(\n",
        "        # read_txt_file(\"meeting_goal.txt\")\n",
        "        interview_data['goal']\n",
        "        + read_txt_file(interview_data['with_name_path']) + \"\"\"\n",
        "        è«‹æ ¹æ“šæ­¤æœƒè­°é€å­—ç¨¿æ¢åˆ—å‡ºæ­¤æœƒè­°çš„é‡é»\"\"\")\n",
        "\n",
        "    write_txt_file(interview_data['summary_path'], response.text)\n",
        "    return interview_data['with_name_path'], interview_data['summary_path']\n",
        "\n",
        "def display_results():\n",
        "    # transcript_path = interview_data['with_name_path']\n",
        "    transcript_path = interview_data['with_name_path']\n",
        "    summary_path = interview_data['summary_path']\n",
        "    with open(transcript_path, 'r', encoding='utf-8') as f:\n",
        "        transcript_content = f.read()\n",
        "\n",
        "    with open(summary_path, 'r', encoding='utf-8') as f:\n",
        "        summary_content = f.read()\n",
        "\n",
        "    return transcript_content, summary_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6lU-_e1W4g9"
      },
      "source": [
        "##UI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"<div style='font-size: 32px;'>Meeting Assistant System</div>\")\n",
        "\n",
        "\n",
        "    # ç¬¬äºŒé ï¼šé€å­—ç¨¿ç”Ÿæˆ\n",
        "    with gr.Tab(\"ç”Ÿæˆé€å­—ç¨¿\"):\n",
        "        generate_button = gr.Button(\"è½‰æˆé€å­—ç¨¿\")\n",
        "        progress_output = gr.Textbox(label=\"é€²åº¦\", value=\"ç­‰å¾…ç”Ÿæˆé€å­—ç¨¿...\", interactive=False)\n",
        "\n",
        "        # ç•¶é»æ“Šç”ŸæˆæŒ‰éˆ•æ™‚ï¼ŒåŸ·è¡Œé€å­—ç¨¿ç”Ÿæˆä¸¦æ›´æ–°é€²åº¦\n",
        "        def generate_transcript():\n",
        "            write_txt_file(interview_data['diarize_path'], '')\n",
        "            write_txt_file(interview_data['llmTranscript_path'], '')\n",
        "\n",
        "            # 1.èªéŸ³æ ¼å¼è½‰æ›æˆwav\n",
        "            yield \"é€²åº¦ï¼šæ ¼å¼è½‰æ›ä¸­...\"\n",
        "            convert_format(interview_data['record_path'], interview_data['wav_path'])\n",
        "            wav_audio = whisperx.load_audio(interview_data['wav_path'])\n",
        "\n",
        "            # 2.èªéŸ³è½‰æ–‡å­—\n",
        "            yield \"é€²åº¦ï¼šèªéŸ³è½‰æ–‡å­—ä¸­...\"\n",
        "            asr_result = transcribe_audio(wav_audio)\n",
        "\n",
        "            # 3.èªè€…æ¨™è¨»\n",
        "            yield \"é€²åº¦ï¼šèªè€…æ¨™è¨»ä¸­...\"\n",
        "            interview_data['speaker_list'] = diarize_audio(wav_audio, asr_result, interview_data['diarize_path'])\n",
        "            # interview_data['speaker_list'] = ['SPEAKER_00', 'SPEAKER_01', 'SPEAKER_02', 'SPEAKER_03', 'SPEAKER_04', 'SPEAKER_05', 'SPEAKER_06']\n",
        "\n",
        "            # 4.é€å­—ç¨¿æ–‡æœ¬è™•ç†(opencc, llm)\n",
        "            yield \"é€²åº¦ï¼šé€å­—ç¨¿æ–‡æœ¬è™•ç†ä¸­...\"\n",
        "            process_transcript(interview_data['diarize_path'], pre_model, model, interview_data['llmTranscript_path'])\n",
        "\n",
        "            # è®€å–ä¸¦é¡¯ç¤ºé€å­—ç¨¿\n",
        "            with open(interview_data['llmTranscript_path'], 'r', encoding='utf-8') as f:\n",
        "                transcript_content = f.read()\n",
        "\n",
        "\n",
        "        generate_button.click(\n",
        "            fn=generate_transcript,\n",
        "            inputs=[],\n",
        "            outputs=[progress_output],\n",
        "            queue=True\n",
        "        )\n",
        "\n",
        "    # ç¬¬ä¸‰é ï¼šèªè€…æ¨™è¨»èˆ‡é€å­—ç¨¿é¡¯ç¤º\n",
        "    with gr.Tab(\"é€å­—ç¨¿èˆ‡èªè€…æ¨™è¨»\"):\n",
        "        # é¡¯ç¤ºé€å­—ç¨¿çš„æŒ‰éˆ•\n",
        "        show_transcript_button = gr.Button(\"é¡¯ç¤ºé€å­—ç¨¿\")\n",
        "\n",
        "        # é€²åº¦é¡¯ç¤ºå€åŸŸ\n",
        "        progress_output = gr.Textbox(label=\"é€²åº¦\", value=\"\", interactive=False, visible=False)\n",
        "\n",
        "        # å·¦å³ä½ˆå±€ï¼Œé€å­—ç¨¿å’Œèªè€…æ¨™è¨»é¸é …\n",
        "        with gr.Row(visible=False) as transcript_section:\n",
        "            # å·¦é‚Šé¡¯ç¤ºé€å­—ç¨¿\n",
        "            transcript_output = gr.Textbox(label=\"é€å­—ç¨¿\", lines=15, interactive=False)\n",
        "\n",
        "            # å³é‚Šèªè€…æ¨™è¨»é¸é …å€åŸŸ\n",
        "            with gr.Column() as speaker_section:\n",
        "                speaker_inputs = [gr.Textbox(label=f\"èªè€… {i+1}\", visible=True) for i in range(10)]\n",
        "                confirm_button = gr.Button(\"ç¢ºèªè¼¸å…¥\", visible=True)\n",
        "\n",
        "        # é¡¯ç¤ºé€å­—ç¨¿æŒ‰éˆ•çš„è¡Œç‚º\n",
        "        def show_transcript():\n",
        "            # è®€å–é€å­—ç¨¿å’Œèªè€…åˆ—è¡¨\n",
        "            # speaker_list = ['SPEAKER_00', 'SPEAKER_01']\n",
        "            try:\n",
        "                with open(interview_data['llmTranscript_path'], 'r', encoding='utf-8') as f:\n",
        "                    transcript_content = f.read()\n",
        "            except FileNotFoundError:\n",
        "                transcript_content = \"ç„¡æ³•æ‰¾åˆ°é€å­—ç¨¿æ–‡ä»¶ï¼Œè«‹ç¢ºèªæ˜¯å¦å·²å®Œæˆé€å­—ç¨¿ç”Ÿæˆã€‚\"\n",
        "\n",
        "            # å‡è¨­èªè€…åˆ—è¡¨å­˜åœ¨æ–¼ interview_data ä¸­\n",
        "            speaker_list = interview_data.get('speaker_list', [])\n",
        "\n",
        "            # æ›´æ–°è¼¸å‡ºå’Œé¡¯ç¤ºé€å­—ç¨¿çš„å€åŸŸ\n",
        "            updates = [\n",
        "                gr.update(visible=True),  # transcript_section é¡¯ç¤º\n",
        "                gr.update(value=transcript_content)  # transcript_output æ›´æ–°ç‚ºé€å­—ç¨¿å…§å®¹\n",
        "            ]\n",
        "\n",
        "            # é¡¯ç¤ºå°æ‡‰èªè€…çš„è¼¸å…¥æ¡†ä»¥åŠç¢ºèªæŒ‰éˆ•\n",
        "            for i in range(len(speaker_list)):\n",
        "                updates.append(gr.update(visible=True, label=f\"èªè€… {i+1}ï¼ˆ{speaker_list[i]}ï¼‰\"))\n",
        "            for i in range(len(speaker_list), len(speaker_inputs)):\n",
        "                updates.append(gr.update(visible=False))  # éš±è—å¤šé¤˜çš„è¼¸å…¥æ¡†\n",
        "            updates.append(gr.update(visible=True))  # ç¢ºèªæŒ‰éˆ•é¡¯ç¤º\n",
        "\n",
        "            return updates\n",
        "\n",
        "        show_transcript_button.click(\n",
        "            fn=show_transcript,\n",
        "            inputs=[],\n",
        "            outputs=[transcript_section, transcript_output] + speaker_inputs + [confirm_button]\n",
        "        )\n",
        "\n",
        "        # ç•¶é»æ“Šç¢ºèªæŒ‰éˆ•æ™‚ï¼Œæ›¿æ›é€å­—ç¨¿ä¸­çš„èªè€…æ¨™ç±¤\n",
        "        def update_speaker_names(transcript_text, *names):\n",
        "            # å»ºç«‹èªè€…æ˜ å°„è¡¨\n",
        "            speaker_map = {f\"SPEAKER_{i:02d}\": name for i, name in enumerate(names) if name.strip()}\n",
        "            # æ›´æ–°é€å­—ç¨¿ï¼Œæ›¿æ›èªè€…åç¨±\n",
        "            updated_transcript = transcript_text\n",
        "            for speaker_label, name in speaker_map.items():\n",
        "                updated_transcript = updated_transcript.replace(speaker_label, name)\n",
        "\n",
        "            write_txt_file(interview_data['with_name_path'], '')\n",
        "            write_txt_file(interview_data['with_name_path'], updated_transcript)\n",
        "            return updated_transcript\n",
        "\n",
        "        confirm_button.click(\n",
        "            fn=update_speaker_names,\n",
        "            inputs=[transcript_output] + speaker_inputs,\n",
        "            outputs=[transcript_output]\n",
        "        )\n",
        "\n",
        "\n",
        "    # ç¬¬å››é ï¼šæœƒè­°æ‘˜è¦èˆ‡åœ˜éšŠåˆ†æ\n",
        "    with gr.Tab(\"æœƒè­°æ‘˜è¦èˆ‡åœ˜éšŠåˆ†æ\"):\n",
        "        with gr.Column():\n",
        "            # å·¦é‚Šï¼šç”Ÿæˆæœƒè­°å¤§ç¶±\n",
        "            summary_output = gr.Textbox(label=\"æœƒè­°å¤§ç¶±èˆ‡åˆ†æ\", interactive=False)\n",
        "            summary_button = gr.Button(\"ç”Ÿæˆæœƒè­°å¤§ç¶±èˆ‡åœ˜éšŠåˆ†æ\")\n",
        "\n",
        "            # # å³é‚Šï¼šåœ˜éšŠåˆ†æ\n",
        "            # analysis_button = gr.Button(\"åœ˜éšŠåˆ†æ\")\n",
        "            # analysis_output = gr.Textbox(label=\"åœ˜éšŠåˆ†æ\", lines=10, interactive=False)\n",
        "\n",
        "        # è¨­ç½®ç”Ÿæˆæœƒè­°å¤§ç¶±çš„æŒ‰éˆ•è¡Œç‚º\n",
        "        def generate_summary():\n",
        "            # æ¨¡æ“¬æœƒè­°å¤§ç¶±ç”Ÿæˆ\n",
        "\n",
        "            response = model.generate_content(\n",
        "              interview_data['goal']\n",
        "              + read_txt_file(interview_data['with_name_path']) + \"\"\"\n",
        "              è«‹æ ¹æ“šæ­¤æœƒè­°é€å­—ç¨¿æ¢åˆ—å‡ºæ­¤æœƒè­°çš„é‡é»\"\"\")\n",
        "\n",
        "            write_txt_file(interview_data['summary_path'], response.text)\n",
        "            summary = read_txt_file(interview_data['summary_path'])\n",
        "            return summary\n",
        "\n",
        "        summary_button.click(\n",
        "            fn=generate_summary,\n",
        "            inputs=[],\n",
        "            outputs=[summary_output]\n",
        "        )\n",
        "\n",
        "        # # è¨­ç½®åœ˜éšŠåˆ†æçš„æŒ‰éˆ•è¡Œç‚º\n",
        "        # def generate_team_analysis():\n",
        "        #     # æ¨¡æ“¬åœ˜éšŠåˆ†æç”Ÿæˆ\n",
        "        #     return \"é€™æ˜¯åœ˜éšŠåˆ†æå ±å‘Šï¼ŒåŒ…æ‹¬èªè€…çš„åƒèˆ‡åº¦å’Œè¨è«–å…§å®¹çš„æ¦‚è¦½ã€‚\"\n",
        "\n",
        "        # analysis_button.click(\n",
        "        #     fn=generate_team_analysis,\n",
        "        #     inputs=[],\n",
        "        #     outputs=[analysis_output]\n",
        "        # )\n",
        "\n",
        "\n",
        "# å•Ÿå‹• Gradio æ‡‰ç”¨\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "EBE7V8CHnrLw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "a28ea4bf-268a-485a-bbc5-b1c9f883f475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gr' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c004c82d2ceb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<div style='font-size: 32px;'>Meeting Assistant System</div>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mspeaker_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpre_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenai_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gr' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "-EzMj2dHHBu7"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}