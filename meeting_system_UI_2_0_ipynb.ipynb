{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ⚙️ 安裝相容版本（降 torch / torchaudio / pyannote）\n",
        "!pip install torch==1.10.0+cu113 torchaudio==0.10.0 -f https://download.pytorch.org/whl/torch_stable.html -q\n",
        "!pip install pytorch-lightning==1.6.5 -q\n",
        "!pip install git+https://github.com/m-bain/whisperx.git -q\n",
        "!pip install pyannote.audio==0.0.1 -q\n",
        "!pip install librosa -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4ypr0eKJU-R",
        "outputId": "fcee8bc1-b7da-4d2d-e7f3-283b8292cd59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.10.0+cu113 (from versions: 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.0+cu117.with.pypi.cudnn, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117, 1.13.1+cu117.with.pypi.cudnn, 2.0.0, 2.0.0+cpu, 2.0.0+cpu.cxx11.abi, 2.0.0+cu117, 2.0.0+cu117.with.pypi.cudnn, 2.0.0+cu118, 2.0.1, 2.0.1+cpu, 2.0.1+cpu.cxx11.abi, 2.0.1+cu117, 2.0.1+cu117.with.pypi.cudnn, 2.0.1+cu118, 2.0.1+rocm5.3, 2.0.1+rocm5.4.2, 2.1.0, 2.1.0+cpu, 2.1.0+cpu.cxx11.abi, 2.1.0+cu118, 2.1.0+cu121, 2.1.0+cu121.with.pypi.cudnn, 2.1.0+rocm5.5, 2.1.0+rocm5.6, 2.1.1, 2.1.1+cpu, 2.1.1+cpu.cxx11.abi, 2.1.1+cu118, 2.1.1+cu121, 2.1.1+cu121.with.pypi.cudnn, 2.1.1+rocm5.5, 2.1.1+rocm5.6, 2.1.2, 2.1.2+cpu, 2.1.2+cpu.cxx11.abi, 2.1.2+cu118, 2.1.2+cu121, 2.1.2+cu121.with.pypi.cudnn, 2.1.2+rocm5.5, 2.1.2+rocm5.6, 2.2.0, 2.2.0+cpu, 2.2.0+cpu.cxx11.abi, 2.2.0+cu118, 2.2.0+cu121, 2.2.0+rocm5.6, 2.2.0+rocm5.7, 2.2.1, 2.2.1+cpu, 2.2.1+cpu.cxx11.abi, 2.2.1+cu118, 2.2.1+cu121, 2.2.1+rocm5.6, 2.2.1+rocm5.7, 2.2.2, 2.2.2+cpu, 2.2.2+cpu.cxx11.abi, 2.2.2+cu118, 2.2.2+cu121, 2.2.2+rocm5.6, 2.2.2+rocm5.7, 2.3.0, 2.3.0+cpu, 2.3.0+cpu.cxx11.abi, 2.3.0+cu118, 2.3.0+cu121, 2.3.0+rocm5.7, 2.3.0+rocm6.0, 2.3.1, 2.3.1+cpu, 2.3.1+cpu.cxx11.abi, 2.3.1+cu118, 2.3.1+cu121, 2.3.1+rocm5.7, 2.3.1+rocm6.0, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.10.0+cu113\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring version 1.6.5 of pytorch-lightning since it has invalid metadata:\n",
            "Requested pytorch-lightning==1.6.5 from https://files.pythonhosted.org/packages/34/de/3c98fb314e5c273a5c8bf0ff3b37e2a2625af7fb6540d9123cd5de975678/pytorch_lightning-1.6.5-py3-none-any.whl has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    torch (>=1.8.*)\n",
            "           ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-lightning==1.6.5 (from versions: 0.0.2, 0.2, 0.2.2, 0.2.3, 0.2.4, 0.2.4.1, 0.2.5, 0.2.5.1, 0.2.5.2, 0.2.6, 0.3, 0.3.1, 0.3.2, 0.3.3, 0.3.4, 0.3.4.1, 0.3.5, 0.3.6, 0.3.6.1, 0.3.6.3, 0.3.6.4, 0.3.6.5, 0.3.6.6, 0.3.6.7, 0.3.6.8, 0.3.6.9, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.6, 0.4.7, 0.4.8, 0.4.9, 0.5.0, 0.5.1, 0.5.1.2, 0.5.1.3, 0.5.2, 0.5.2.1, 0.5.3, 0.5.3.1, 0.5.3.2, 0.5.3.3, 0.6.0, 0.7.1, 0.7.3, 0.7.5, 0.7.6, 0.8.1, 0.8.3, 0.8.4, 0.8.5, 0.9.0, 0.10.0, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.0.6, 1.0.7, 1.0.8, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5, 1.1.6, 1.1.7, 1.1.8, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.2.4, 1.2.5, 1.2.6, 1.2.7, 1.2.8, 1.2.9, 1.2.10, 1.3.0rc1, 1.3.0rc2, 1.3.0rc3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5, 1.3.6, 1.3.7, 1.3.7.post0, 1.3.8, 1.4.0rc0, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.4.2, 1.4.3, 1.4.4, 1.4.5, 1.4.6, 1.4.7, 1.4.8, 1.4.9, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.5.5, 1.5.6, 1.5.7, 1.5.8, 1.5.9, 1.5.10, 1.5.10.post0, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.6.1, 1.6.2, 1.6.3, 1.6.4, 1.6.5, 1.6.5.post0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.7.4, 1.7.5, 1.7.6, 1.7.7, 1.8.0rc0, 1.8.0rc1, 1.8.0rc2, 1.8.0, 1.8.0.post1, 1.8.1, 1.8.2, 1.8.3, 1.8.3.post0, 1.8.3.post1, 1.8.3.post2, 1.8.4, 1.8.4.post0, 1.8.5, 1.8.5.post0, 1.8.6, 1.9.0rc0, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.9.4, 1.9.5, 2.0.0rc0, 2.0.0, 2.0.1, 2.0.1.post0, 2.0.2, 2.0.3, 2.0.4, 2.0.5, 2.0.6, 2.0.7, 2.0.8, 2.0.9, 2.0.9.post0, 2.1.0rc0, 2.1.0rc1, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0rc0, 2.2.0, 2.2.0.post0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.4.0, 2.5.0rc0, 2.5.0, 2.5.0.post0, 2.5.1rc0, 2.5.1rc1, 2.5.1rc2, 2.5.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch-lightning==1.6.5\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "whisperx 3.3.1 requires ctranslate2<4.5.0, but you have ctranslate2 4.5.0 which is incompatible.\n",
            "whisperx 3.3.1 requires faster-whisper==1.1.0, but you have faster-whisper 1.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 2.0.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchaudio<1.0,>=0.10 (from pyannote-audio) (from versions: 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchaudio<1.0,>=0.10\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔧 重新啟動 runtime 之後再執行這段！\n",
        "\n",
        "import whisperx\n",
        "import torch\n",
        "from google.colab import files\n",
        "\n",
        "# ⚙️ 設定 device / model / type\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "batch_size = 8\n",
        "\n",
        "# 📁 上傳音訊檔案\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]  # 只取第一個上傳的檔案\n",
        "print(f\"已上傳檔案：{file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "GisW608_JWGg",
        "outputId": "65549ae2-4649-433f-db94-af315c9cac51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Only a single TORCH_LIBRARY can be used to register the namespace prims; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:241; latest registration was registered at /dev/null:241",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8cb53a5e00b8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 🔧 重新啟動 runtime 之後再執行這段！\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwhisperx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisperx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malignment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_align_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdiarize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massign_word_speakers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiarizationPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisperx/transcribe.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malignment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_align_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymBool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from torch._decomp import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0m_convert_out_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;31m# populate the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/decompositions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_prims/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prims\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DEF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprim_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prims\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IMPL\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CompositeExplicitAutograd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprim_backend_select_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prims\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IMPL\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"BackendSelect\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/library.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ns, kind, dispatch_key)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         self.m: Optional[Any] = torch._C._dispatch_library(\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispatch_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Only a single TORCH_LIBRARY can be used to register the namespace prims; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:241; latest registration was registered at /dev/null:241"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisperx\n",
        "import gc\n",
        "\n",
        "device = \"cuda\"\n",
        "audio_file = \"/content/新的錄音 91.m4a\" #\"audio.mp3\"\n",
        "batch_size = 16 # reduce if low on GPU mem\n",
        "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
        "\n",
        "# 1. Transcribe with original whisper (batched)\n",
        "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
        "\n",
        "# save model to local path (optional)\n",
        "# model_dir = \"/path/\"\n",
        "# model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, download_root=model_dir)\n",
        "\n",
        "audio = whisperx.load_audio(audio_file)\n",
        "result = model.transcribe(audio, batch_size=batch_size)\n",
        "print(result[\"segments\"]) # before alignment\n",
        "\n",
        "# delete model if low on GPU resources\n",
        "# import gc; gc.collect(); torch.cuda.empty_cache(); del model\n",
        "\n",
        "# 2. Align whisper output\n",
        "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
        "\n",
        "print(result[\"segments\"]) # after alignment\n",
        "\n",
        "# delete model if low on GPU resources\n",
        "# import gc; gc.collect(); torch.cuda.empty_cache(); del model_a\n",
        "\n",
        "# 3. Assign speaker labels\n",
        "diarize_model = whisperx.DiarizationPipeline(use_auth_token=\"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\", device=device)\n",
        "\n",
        "# add min/max number of speakers if known\n",
        "diarize_segments = diarize_model(audio)\n",
        "# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)\n",
        "\n",
        "result = whisperx.assign_word_speakers(diarize_segments, result)\n",
        "print(diarize_segments)\n",
        "print(result[\"segments\"]) # segments are now assigned speaker IDs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKiktezUD4GQ",
        "outputId": "f6dab77a-b6dc-45dc-98c7-288d2d858fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../usr/local/lib/python3.11/dist-packages/whisperx/assets/pytorch_model.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cu124. Bad things might happen unless you revert torch to 1.x.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
            "It can be re-enabled by calling\n",
            "   >>> import torch\n",
            "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
            "   >>> torch.backends.cudnn.allow_tf32 = True\n",
            "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XM3NGXugAP65",
        "outputId": "a5ef8c21-7c59-4aa1-8b71-da884351b61e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencc\n",
            "  Downloading OpenCC-1.1.9-cp311-cp311-manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading OpenCC-1.1.9-cp311-cp311-manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.7 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencc\n",
            "Successfully installed opencc-1.1.9\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m338.6/363.4 MB\u001b[0m \u001b[31m143.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install opencc\n",
        "!pip install tiktoken\n",
        "!pip install --q git+https://github.com/m-bain/whisperX.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-multipart"
      ],
      "metadata": {
        "id": "RG3ApoajGx5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca35f9e-88ff-4053-a2a0-e36705b486e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU63KRxmBvSy"
      },
      "source": [
        "##Collecting interview data\n",
        "使用者輸入的資訊在此存為global variable，如下：  \n",
        "interview_data['title']  \n",
        "interview_data['max_speakers']  \n",
        "interview_data['min_speakers']  \n",
        "interview_data['outline']  \n",
        "interview_data['record_path']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcW0rwDB2x6k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "19d9d8f9-ca25-41cc-e671-f7e00b97a373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "請輸入會議名稱：1\n",
            "請輸入最大發言人數：2\n",
            "請輸入最小發言人數：2\n",
            "請上傳訪綱(.txt/.pdf/.doc)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4b521666-f36e-4a97-aba7-03c0a6e6c682\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4b521666-f36e-4a97-aba7-03c0a6e6c682\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving outline_file.pdf to outline_file (1).pdf\n",
            "請上傳訪談錄音檔（例如 .wav 或 .mp3）\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a8df2144-4ff6-4538-8d86-c4ccba35bbea\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a8df2144-4ff6-4538-8d86-c4ccba35bbea\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 新的錄音 91.m4a to 新的錄音 91.m4a\n",
            "已上傳檔案: outline_file (1).pdf\n",
            "訪綱已儲存為: /content/outline_file.pdf\n",
            "已上傳檔案: 新的錄音 91.m4a\n",
            "錄音檔案已儲存為: /content/record_file.m4a\n",
            "\n",
            "✅ 資料已儲存完成\n",
            "會議名稱: 1\n",
            "發言人數範圍: 2 ~ 2\n",
            "訪談大綱: /content/outline_file.pdf\n",
            "影音檔案路徑: /content/record_file.m4a\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# 全局變數，用來存儲會議資料\n",
        "interview_data = {}\n",
        "\n",
        "interview_data['title'] = \"\"\n",
        "interview_data['max_speakers'] = 0\n",
        "interview_data['min_speakers'] = 0\n",
        "interview_data['outline_ext'] = \"\"\n",
        "interview_data['outline_path'] = \"\"\n",
        "interview_data['record_ext'] = \"\"\n",
        "interview_data['record_path'] = \"\"\n",
        "interview_data['wav_path'] = \"/content/upload_file.wav\"\n",
        "interview_data['transcript_path'] = \"/content/asr.txt\"\n",
        "interview_data['diarize_path'] = \"/content/diarize.txt\"\n",
        "interview_data['summary_path'] = \"/content/summary.txt\"\n",
        "interview_data['llmTranscript_path'] = \"/content/llmTranscript.txt\"\n",
        "interview_data['with_name_path'] = \"/content/with_name.txt\"\n",
        "\n",
        "# 1. 讓使用者輸入基本資訊（你也可以用 input() 函數）\n",
        "interview_data['title'] = input(\"請輸入會議名稱：\")\n",
        "interview_data['max_speakers'] = int(input(\"請輸入最大發言人數：\"))\n",
        "interview_data['min_speakers'] = int(input(\"請輸入最小發言人數：\"))\n",
        "print(\"請上傳訪綱(.txt/.pdf/.doc)\")\n",
        "uploaded_outline = files.upload()\n",
        "print(\"請上傳訪談錄音檔（例如 .wav 或 .mp3）\")\n",
        "uploaded_record = files.upload()\n",
        "\n",
        "# 2. 處理上傳的檔案\n",
        "for filename in uploaded_outline.keys():\n",
        "    print(f\"已上傳檔案: {filename}\")\n",
        "    file_ext = os.path.splitext(filename)[-1].lower()\n",
        "    interview_data['outline_ext'] = file_ext\n",
        "    save_path = f\"/content/outline_file{file_ext}\"\n",
        "    shutil.move(filename, save_path)\n",
        "    interview_data['outline_path'] = save_path\n",
        "\n",
        "    print(f\"訪綱已儲存為: {save_path}\")\n",
        "\n",
        "for filename in uploaded_record.keys():\n",
        "    print(f\"已上傳檔案: {filename}\")\n",
        "    file_ext = os.path.splitext(filename)[-1].lower()\n",
        "    interview_data['record_ext'] = file_ext\n",
        "    save_path = f\"/content/record_file{file_ext}\"\n",
        "    shutil.move(filename, save_path)\n",
        "    interview_data['record_path'] = save_path\n",
        "\n",
        "    print(f\"錄音檔案已儲存為: {save_path}\")\n",
        "\n",
        "# 4. 確認資訊\n",
        "print(\"\\n✅ 資料已儲存完成\")\n",
        "print(f\"會議名稱: {interview_data['title']}\")\n",
        "print(f\"發言人數範圍: {interview_data['min_speakers']} ~ {interview_data['max_speakers']}\")\n",
        "print(f\"訪談大綱: {interview_data['outline_path']}\")\n",
        "print(f\"影音檔案路徑: {interview_data['record_path']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-cN-QxoAg2r"
      },
      "source": [
        "# **Part1. Audio to text by WhisperX**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbkGfNJaAU2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f767386d-2fa0-4c9b-aac1-0e22776f1a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.1.11)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.0.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.1.31)\n",
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6083 sha256=d117558d603df352a04c3218ee20de48c77fe7a1ed04d3e2b0e68fc60bee5cd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/30/c5/576bdd729f3bc062d62a551be7fefd6ed2f761901568171e4e\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "import whisperx\n",
        "import gc\n",
        "device = \"cuda\"\n",
        "batch_size = 6 # reduce if low on GPU mem\n",
        "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install moviepy\n",
        "!pip install ffmpeg\n",
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisperx\n",
        "import gc\n",
        "device = \"cuda\"\n",
        "batch_size = 6 # reduce if low on GPU mem\n",
        "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "fv52q8Ix3ioy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "將錄音檔統一轉換為.wav格式"
      ],
      "metadata": {
        "id": "gd0i4iwGw0Iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "import mimetypes\n",
        "\n",
        "audio_file = interview_data['record_path']\n",
        "file_ext = interview_data['record_ext']\n",
        "\n",
        "# 如果文件不是wav格式，则转换为wav\n",
        "if file_ext != \".wav\":\n",
        "    print(f\"Converting {audio_file} to wav format...\")\n",
        "\n",
        "    # 使用pydub加载音频文件\n",
        "    audio = AudioSegment.from_file(audio_file)\n",
        "\n",
        "    # 转换后的wav文件路径\n",
        "    path = os.path.splitext(audio_file)[0] + \".wav\"\n",
        "\n",
        "    # 導出wav格式\n",
        "    audio.export(path, format=\"wav\")\n",
        "    interview_data['record_path'] = path\n",
        "    interview_data['record_ext'] = \".wav\"\n",
        "    print(f\"Converted file saved as: {interview_data['record_path']}\")\n",
        "else:\n",
        "    print(f\"Record file is already in wav format.\")\n",
        "\n",
        "print(f\"Audio loaded from {interview_data['record_path']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pR-5D7fxQ06",
        "outputId": "4a8cd148-3505-4d9b-fcd1-823ebe0fd497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting /content/record_file.m4a to wav format...\n",
            "Converted file saved as: /content/record_file.wav\n",
            "Audio loaded from /content/record_file.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "轉成逐字稿"
      ],
      "metadata": {
        "id": "zKEJ9ICu0TY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y pyannote.audio\n",
        "!pip install pyannote.audio==0.0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ydWGM4y4Ett",
        "outputId": "f9d7000c-aaec-406a-f107-7f2f4858eb3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pyannote.audio 3.3.2\n",
            "Uninstalling pyannote.audio-3.3.2:\n",
            "  Successfully uninstalled pyannote.audio-3.3.2\n",
            "Collecting pyannote.audio==0.0.1\n",
            "  Downloading pyannote.audio-0.0.1-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: asteroid-filterbanks<0.5,>=0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==0.0.1) (0.4.0)\n",
            "Collecting backports.cached-property (from pyannote.audio==0.0.1)\n",
            "  Downloading backports.cached_property-1.0.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting einops<0.4.0,>=0.3 (from pyannote.audio==0.0.1)\n",
            "  Downloading einops-0.3.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting hmmlearn<0.3,>=0.2.7 (from pyannote.audio==0.0.1)\n",
            "  Downloading hmmlearn-0.2.8.tar.gz (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub<0.9,>=0.7 (from pyannote.audio==0.0.1)\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting networkx<3.0,>=2.6 (from pyannote.audio==0.0.1)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==0.0.1) (2.3.0)\n",
            "Collecting pyannote.core<5.0,>=4.4 (from pyannote.audio==0.0.1)\n",
            "  Downloading pyannote.core-4.5-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyannote.database<5.0,>=4.1.1 (from pyannote.audio==0.0.1)\n",
            "  Downloading pyannote.database-4.1.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pyannote.metrics<4.0,>=3.2 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==0.0.1) (3.2.1)\n",
            "Collecting pyannote.pipeline<3.0,>=2.3 (from pyannote.audio==0.0.1)\n",
            "  Downloading pyannote.pipeline-2.3-py3-none-any.whl.metadata (955 bytes)\n",
            "Collecting pytorch-lightning<1.7,>=1.5.4 (from pyannote.audio==0.0.1)\n",
            "  Downloading pytorch_lightning-1.6.5.post0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting pytorch-metric-learning<2.0,>=1.0.0 (from pyannote.audio==0.0.1)\n",
            "  Downloading pytorch_metric_learning-1.7.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting semver<3.0,>=2.10.2 (from pyannote.audio==0.0.1)\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting singledispatchmethod (from pyannote.audio==0.0.1)\n",
            "  Downloading singledispatchmethod-1.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting soundfile<0.11,>=0.10.2 (from pyannote.audio==0.0.1)\n",
            "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting speechbrain<0.6,>=0.5.12 (from pyannote.audio==0.0.1)\n",
            "  Downloading speechbrain-0.5.16-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==0.0.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: torch-audiomentations>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==0.0.1) (0.12.0)\n",
            "INFO: pip is looking at multiple versions of pyannote-audio to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 2.0.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchaudio<1.0,>=0.10 (from pyannote-audio) (from versions: 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchaudio<1.0,>=0.10\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W33j6kobA8Aw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "b2674269-1edb-4824-bc68-0fe96f05951e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'whisperx' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2eaaa70e91cf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# audio = whisperx.load_audio(interview_data['record_path'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisperx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/record_file.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisperx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"large-v2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 未經任何切割\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'whisperx' is not defined"
          ]
        }
      ],
      "source": [
        "# audio = whisperx.load_audio(interview_data['record_path'])\n",
        "audio = whisperx.load_audio('/content/record_file.wav')\n",
        "\n",
        "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
        "result = model.transcribe(audio, batch_size=batch_size)   # 未經任何切割\n",
        "\n",
        "# 2. Align whisper output\n",
        "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "asr_result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebr58_J0C_aQ"
      },
      "source": [
        "# **Part2. Speaker Diarization by WhisperX**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N25Yqo48C6Ik"
      },
      "outputs": [],
      "source": [
        "def read_txt_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def write_txt_file(file_path, content):\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "\n",
        "def process_result(result):\n",
        "    transcript = []\n",
        "    pure_transcript = []\n",
        "    pure_speaker=[]\n",
        "    current_speaker = None\n",
        "    current_sentence = []\n",
        "\n",
        "    for i, segment in enumerate(result['word_segments']):\n",
        "        # print(segment)\n",
        "        word = segment['word']\n",
        "\n",
        "        # 處理資料缺失的問題，若一個 word 沒有對應的 speaker，取最近的 speaker\n",
        "        if 'speaker' not in segment:\n",
        "            j = i + 1\n",
        "            while j < len(result['word_segments']) and 'speaker' not in result['word_segments'][j]:\n",
        "                j += 1\n",
        "            if j < len(result['word_segments']):\n",
        "                speaker = result['word_segments'][j]['speaker']\n",
        "            else:\n",
        "                speaker = current_speaker\n",
        "        else:\n",
        "            speaker = segment['speaker']\n",
        "\n",
        "        if speaker != current_speaker:\n",
        "            if current_sentence:\n",
        "                transcript.append(f\"{current_speaker}: {''.join(current_sentence)}\")\n",
        "                pure_transcript.append(f\"{''.join(current_sentence)}\")\n",
        "                pure_speaker.append(f\"{current_speaker}\")\n",
        "                current_sentence = []\n",
        "            current_speaker = speaker\n",
        "\n",
        "        current_sentence.append(word)\n",
        "\n",
        "    if current_sentence:\n",
        "        transcript.append(f\"{current_speaker}: {''.join(current_sentence)}\")\n",
        "        pure_transcript.append(f\"{''.join(current_sentence)}\")\n",
        "        pure_speaker.append(f\"{current_speaker}\")\n",
        "\n",
        "    return '\\n'.join(transcript), '\\n'.join(pure_transcript), '\\n'.join(pure_speaker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0Wyu323g8tl"
      },
      "outputs": [],
      "source": [
        "def diarize_audio(audio, asr_result, path):\n",
        "    diarize_model = whisperx.DiarizationPipeline(\n",
        "        use_auth_token=\"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\", device=device\n",
        "    )\n",
        "    diarize_segments = diarize_model(\n",
        "        audio, min_speakers=interview_data['min_speakers'], max_speakers=interview_data['max_speakers']\n",
        "    )\n",
        "    result = whisperx.assign_word_speakers(diarize_segments, asr_result)\n",
        "\n",
        "    # 提取所有的語者標籤\n",
        "    speaker_list = diarize_segments['speaker'].unique().tolist()  # 獲取唯一的語者標籤並轉為列表\n",
        "    speaker_list = sorted(speaker_list)  # 排序，以便後續處理\n",
        "\n",
        "    print(speaker_list)\n",
        "\n",
        "    # 將逐字稿轉換為文本\n",
        "    transcript = process_result(result)[0]\n",
        "    write_txt_file(path, transcript)\n",
        "\n",
        "    return speaker_list  # 返回語者列表，用於後續標註"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interview_data['speaker_list'] = diarize_audio(audio, asr_result, interview_data['diarize_path'])"
      ],
      "metadata": {
        "id": "wUTcXnvm2wWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EzMj2dHHBu7"
      },
      "source": [
        "# **Part3**. **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yEsu0O3LqkC"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "# import os\n",
        "def genai_setup():\n",
        "  api_key = 'AIzaSyBH3SedO17XmiLer4xdRCIqVb_0-icHJFg'   # Ching's api key\n",
        "  genai.configure(api_key = api_key)\n",
        "\n",
        "  pre_model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", system_instruction=\n",
        "    \"\"\"\n",
        "    你是一個不會生成出簡體字或中國用語、且擅長進行錯字與用詞校正和標點符號標註的台灣文字工作者。\n",
        "    你會協助將輸入的逐字稿完成錯字修正與用詞校正，並將其標註邊點符號。\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "  model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    system_instruction=\n",
        "    \"\"\"\n",
        "    你是一個不會生成出簡體字或中國用語、且很會做摘要的台灣會議師。你會仔細的研讀會議內容，並根據討論的內容有條理地整理出會議片段的重點。\n",
        "\n",
        "  以下是會議重點條列的模板:\n",
        "  '''\n",
        "  1. 會議開始與教授出席：\n",
        "  - 教授確認會議開始，並詢問是否需要等待其他成員到齊。\n",
        "  2. 進度報告：\n",
        "  - 教授已查看進度統整，並請團隊分享未在進度報告中提到的細節。\n",
        "  - 強調需要數字化的資料來評估進展，而不僅是敘述。\n",
        "  3. 現有LLM的評估：\n",
        "  - 團隊報告目前使用的LLM模型及其性能，教授要求詳細的數據和測試方法。\n",
        "  - 教授詢問模型的應用場景和具體使用情況，並要求提供數據來進行性能分析。\n",
        "  4. LLM性能指標分析：\n",
        "  - 教授建議統計模型的準確率、效能、處理時間等數據，以便了解不同情況下的性能表現。\n",
        "  5. 團隊提出問題：\n",
        "  - 團隊詢問教授是否有推薦的LLM模型，教授分享不同LLM的特點和優缺點。\n",
        "  6. 資料來源：\n",
        "  - 教授提到可以使用開源數據集進行測試，並詢問團隊是否需要額外的數據來源。\n",
        "  7. LLM模型與工具：\n",
        "  - 團隊詢問教授是否有推薦的LLM模型，教授建議使用GPT-4作為基礎，並探討其他可能的模型如BERT、T5等。\n",
        "  8. 模型調整與優化：\n",
        "  - 團隊成員展示如何調整模型參數以提高性能，但結果不如預期。\n",
        "  - 教授建議根據具體應用場景調整模型參數，並使用不同的優化技術。\n",
        "  9. 團隊互助：\n",
        "  - 教授鼓勵不同團隊之間互相幫助，共同解決問題。\n",
        "  10. 下一步行動：\n",
        "  - 團隊需要整理並分享詳細的測試數據，進行更深入的性能分析。\n",
        "  - 探索使用不同LLM模型進行更多測試。\n",
        "  - 根據應用場景調整和優化LLM模型的參數。\n",
        "  '''\n",
        "  如果有專有名詞話要記的包括進去。例如有\"GPT-4\"的話就不要只說\"LLM\"或\"大型語言模型\"。\n",
        "\n",
        "  請進行「貝爾賓團隊角色分析」，請分析文本中每個人在對話中所扮演的角色有哪些團隊角色的特質。並且描述這個團隊的優勢與弱勢。貝爾賓團隊腳色理論的介紹如下:\n",
        "  貝爾賓團隊角色理論由貝爾賓(Meredith Belbin)博士提出，將角色區分為行動導向型、謀略導向型及人際導向型三大類別，從而取得團隊的平衡並提高生產力:\n",
        "    1.形塑者：屬於行動導向型，型塑者是將團隊向前推動的團隊成員，在發生任何問題時仍能鼓舞自己和其他人。型塑者是天生的領導者，因此在管理角色中長袖善舞。發生危機時，他們能很快找到解決方案。\n",
        "    2.執行者：屬於行動導向型，他們會在自己的環境中維持秩序。這類型的成員很務實，善於實現構想。雖然執行者喜歡採取行動，但他們也高度自律。由於他們可以很有自信地支援其他團隊成員，因此可以成為團隊的支柱。\n",
        "    3.完成者：屬於行動導向型，完成者是腳踏實地、留意精巧細節並力求完美的成員。這類團隊成員可能較為內向，然而，由於他們會敦促團隊成員產出優質的工作成果，因此在工作環境中發揮重要的價值。\n",
        "    4.創新者：屬於謀略導向型，他們是創新而創意十足的思維者。儘管創新者有助平衡團隊結構，他們在與大團隊分享自己的構想前，偏好透過腦力激盪使構想具體成形。創新者可能會偏好獨自作業，但即使他們不如其他團隊成員那麼侃侃而談，卻仍為團隊帶來珍貴的貢獻。\n",
        "    5.監察員：屬於謀略導向型，此類型是理性思維者，可將情緒擺一邊，專心解決問題。當專案需要進階知識和策略性規劃時，監察員就能獲得最佳發揮。他們會審評構想，判斷這些構想的價值和可行性，接著再採取步驟將構想向前推動。\n",
        "    6.專家：屬於謀略導向型，對其各自領域具備深度知識，他們喜歡針對一個領域的專業做出貢獻。專家會依循所有謀略導向型團隊角色的模式，因為相較於在團隊中工作，他們獨自作業時表現更為出色。儘管此類型成員獨立性較高，但他們卻能透過具體的綜合技能為團隊創造偌大價值。\n",
        "    7.協調者：屬於人際導向型，協調者是具備絕佳溝通技能的團隊成員。通常協調者會擔任領導職，因為他們能促進協作並激勵團隊達成目標。其他團隊成員尊敬協調者並信任他們，放心交由他們做決策。\n",
        "    8.團隊工作者：屬於人際導向型，外向性格有助於他們與他人合作愉快，並且聆聽隊友的意見。這類團隊成員能輕易在環境中因應變化而調整，而且他們知道發生衝突時應如何營造和諧氣氛。若有團隊成員手上的工作太多，或遭遇家人相關的緊急事件，團隊工作者會是挺身而出、提供支援的第一人。\n",
        "    9.資源調查者：屬於人際導向型，樂於探索新的機會，例如為公司尋找有潛力的行銷機會，或鼓動專案關係人發佈新產品。正向積極的態度使他們成為天生的人脈專家，或新業務的催生者。\n",
        "  以下是各個團隊角色的範例:\n",
        "    1.型塑者團隊角色範例：若用產品行銷團隊作為例子來說明，型塑者就是產品總監，由他們監督團隊達成目標的願景和藍圖。\n",
        "    2.執行者團隊角色範例：執行者會是產品行銷團隊的商業分析師，他們以資料作為分析的依據，審評讓組織流程更有效率的各種方式。\n",
        "    3.完成者團隊角色範例：完成者在技術支援的工作上游刃有餘。他們知道如何快速而有效率地找出並解決問題。\n",
        "    4.創新者團隊角色範例：創新者具有高度創造力，因此能成為優秀的產品設計師。\n",
        "    5.監察員團隊角色範例：監察員會是超級有系統的專案經理，他們以策略性角度出發，界定專案的範疇並將團隊的點狀事務串聯成線。\n",
        "    6.專家團隊角色範例：專家可能會是程式設計師、SEO (搜尋引擎最佳化) 分析師，或團隊的技術小組，他們的職責是專精於團隊其他人可能不甚熟悉的某種技能，而這部分的工作也多虧有他們，才能免於出錯。\n",
        "    7.協調者團隊角色範例：協調者樂於協調和鼓舞其他人，因此可勝任產品開發人員的團隊主管。\n",
        "    8.團隊工作者團隊角色範例：由於團隊工作者是天生的協作者，他們在大型團隊中擔任產品行銷人員，會有卓越的表現。\n",
        "    9.資源調查者範例：由於資源調查者喜歡與其他人建立人脈，因此在產品銷售的領域可以游刃有餘。\n",
        "\n",
        "  最後，請回答這個會議有沒有進入一個鬼打牆的情況?\n",
        "    \"\"\")\n",
        "  return pre_model, model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu3XWjVTmiZF"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import jieba\n",
        "import opencc\n",
        "import tiktoken\n",
        "\n",
        "# 自定义冗言赘字词库\n",
        "redundant_words = set([\n",
        "    \"就是\", \"然後\", \"但是\", \"所以\", \"其實\", \"那麼\", \"就是說\", \"這樣\", \"那樣\", \"對吧\", \"這個\", \"那個\", \"啊\", \"嗯\", \"哦\", \"呃\", \"嘿\", \"嘛\", \"哼\"\n",
        "])\n",
        "\n",
        "# 自定義的校正字典\n",
        "correction_dict = {\n",
        "    '爲': '為',\n",
        "\t  '喫': '吃',\n",
        "    '裏': '裡',\n",
        "    '牀': '床',\n",
        "    '纔': '才',\n",
        "    '僞': '偽',\n",
        "    '衆': '眾',\n",
        "    '孃': '娘',\n",
        "    '彆': '別'\n",
        "}\n",
        "\n",
        "speaker_map = {}\n",
        "\n",
        "def write_file(file_path, content):\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write('\\n'.join(content))\n",
        "\n",
        "def write_file_add(file_path, content):\n",
        "    with open(file_path, 'a', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "\n",
        "def remove_redundant_words(text):\n",
        "    words = jieba.cut(text)\n",
        "    cleaned_words = [word for word in words if word not in redundant_words]\n",
        "    return ''.join(cleaned_words)\n",
        "\n",
        "def correct_text(text, correction_dict):\n",
        "    \"\"\"根據字典對文本進行校正\"\"\"\n",
        "    for original, corrected in correction_dict.items():\n",
        "        text = text.replace(original, corrected)\n",
        "    return text\n",
        "\n",
        "\n",
        "def process_transcript(input_file, pre_model, model, path):\n",
        "    # 創建簡體中文轉繁体中文的轉换器\n",
        "    converter = opencc.OpenCC('s2t')\n",
        "\n",
        "    # 初始化 tiktoken 编码器\n",
        "    encoder = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    processed_lines = []\n",
        "    # timestamp_pattern = re.compile(r'\\[.*?\\]')\n",
        "    speaker_pattern = re.compile(r'SPEAKER_\\d+')\n",
        "\n",
        "    for line in lines:\n",
        "        # Remove timestamps\n",
        "        # line = re.sub(timestamp_pattern, '', line).strip()\n",
        "\n",
        "        # Replace speaker labels with provided names\n",
        "        match = re.search(speaker_pattern, line)\n",
        "        if match:\n",
        "            speaker_label = match.group(0)\n",
        "            speaker_map[speaker_label] = speaker_label + '_'\n",
        "\n",
        "        # Convert simplified Chinese to traditional Chinese\n",
        "        line = converter.convert(line)\n",
        "\n",
        "        line = correct_text(line, correction_dict)\n",
        "\n",
        "        # Remove redundant words\n",
        "        line = remove_redundant_words(line)\n",
        "\n",
        "        # Replace spaces with commas, except after colons\n",
        "        # Replace spaces with commas, except after colons\n",
        "\n",
        "        if line:  # Only add non-empty lines\n",
        "            processed_lines.append(line)  # Add period at the end of each line\n",
        "\n",
        "    # Split lines into multiple files based on token count\n",
        "    max_tokens = 8192 #4096 2048\n",
        "    current_tokens = 0\n",
        "    file_index = 0\n",
        "    current_file_lines = []\n",
        "\n",
        "    for line in processed_lines:\n",
        "        tokens = encoder.encode(line)\n",
        "        token_count = len(tokens)\n",
        "\n",
        "        if current_tokens + token_count > max_tokens:\n",
        "            # Write current file\n",
        "            # write_file(f\"{output_file_prefix}_{file_index}.txt\", current_file_lines)\n",
        "\n",
        "            response = pre_model.generate_content(current_file_lines)\n",
        "\n",
        "            write_file_add(path, response.text)\n",
        "\n",
        "            # Reset for next file\n",
        "            file_index += 1\n",
        "            current_file_lines = [line]\n",
        "            current_tokens = token_count\n",
        "        else:\n",
        "            current_file_lines.append(line)\n",
        "            current_tokens += token_count\n",
        "\n",
        "    # Write the last file\n",
        "    if current_file_lines:\n",
        "        # write_file(f\"{output_file_prefix}_{file_index}.txt\", current_file_lines)\n",
        "        response = pre_model.generate_content(current_file_lines)\n",
        "\n",
        "        write_file_add(path, response.text)\n",
        "\n",
        "\n",
        "# # input_file = '0709.txt'\n",
        "# pre_model, model = genai_setup()\n",
        "# process_transcript(interview_data['diarize_path'], pre_model, model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLoxA5pFHkQO"
      },
      "source": [
        "# **Part4. Role Analysis And Summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIxiqjFOMIAS"
      },
      "outputs": [],
      "source": [
        "# '''def read_file(file_path):\n",
        "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
        "#         content = file.read()\n",
        "#     return content'''\n",
        "\n",
        "# def map_speaker(input_file, map):\n",
        "\n",
        "#     speaker_pattern = re.compile(r'SPEAKER_\\d+')\n",
        "\n",
        "#     with open(input_file, 'r', encoding='utf-8') as file:\n",
        "#         lines = file.readlines()\n",
        "\n",
        "#     for line in lines:\n",
        "#       # Replace speaker labels with provided names\n",
        "#       match = re.search(speaker_pattern, line)\n",
        "#       if match:\n",
        "#           speaker_label = match.group(0)\n",
        "#           if speaker_label in map:\n",
        "#               line = line.replace(speaker_label, map[speaker_label])\n",
        "\n",
        "#       if line:  # Only add non-empty lines\n",
        "#           write_file_add(interview_data['with_name_path'], line)\n",
        "\n",
        "# # user 輸入\n",
        "# speaker_mapping = {\n",
        "#         \"SPEAKER_00_\": \"可親\",\n",
        "#         \"SPEAKER_01_\": \"東廷\",\n",
        "#         \"SPEAKER_02_\": \"Uray\",\n",
        "#         \"SPEAKER_03_\": \"秉諺\",\n",
        "#         \"SPEAKER_05\": \"宇婕\",\n",
        "#         \"SPEAKER_06\": \"教授\",\n",
        "#     }\n",
        "\n",
        "# # 校正後會議逐字稿\n",
        "# # input_file = interview_data['llmTranscript_path']\n",
        "\n",
        "import re\n",
        "\n",
        "def map_speaker(input_file, speaker_map, path):\n",
        "    # 正则表达式：匹配 SPEAKER_ 后面跟着数字的标签\n",
        "    speaker_pattern = re.compile(r'SPEAKER_\\d+?')\n",
        "\n",
        "    # 读取文件内容\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # 替换并写入文件\n",
        "    with open(path, 'w', encoding='utf-8') as output_file:\n",
        "        for line in lines:\n",
        "            match = re.search(speaker_pattern, line)\n",
        "            if match:\n",
        "                speaker_label = match.group(0)\n",
        "                if speaker_label in speaker_map:\n",
        "                    # 使用映射中的名字替换\n",
        "                    line = line.replace(speaker_label, speaker_map[speaker_label])\n",
        "            output_file.write(line)\n",
        "\n",
        "# 用户输入的 speaker 映射\n",
        "speaker_mapping = {\n",
        "    \"SPEAKER_00\": \"可親\",\n",
        "    \"SPEAKER_01\": \"東廷\",\n",
        "    \"SPEAKER_02\": \"Uray\",\n",
        "    \"SPEAKER_03\": \"秉諺\",\n",
        "    \"SPEAKER_05\": \"宇婕\",\n",
        "    \"SPEAKER_06\": \"教授\",\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyDtWI0MXZQ7"
      },
      "outputs": [],
      "source": [
        "# 模擬分析函數，會進行一些耗時的後端運算\n",
        "def start_analysis():\n",
        "    convert_format(interview_data['record_path'], interview_data['wav_path'])\n",
        "    wav_audio = whisperx.load_audio(interview_data['wav_path'])\n",
        "    asr_result = transcribe_audio(wav_audio)     # get .wav and ASR result\n",
        "    speaker_list = diarize_audio(wav_audio, asr_result, interview_data['diarize_path'])\n",
        "    print(speaker_list)\n",
        "    pre_model, model = genai_setup()\n",
        "    process_transcript(interview_data['diarize_path'], pre_model, model, interview_data['llmTranscript_path'])\n",
        "\n",
        "    # map_speaker(interview_data['llmTranscript_path'], speaker_map, interview_data['with_name_path'])\n",
        "    response = model.generate_content(\n",
        "        # read_txt_file(\"meeting_goal.txt\")\n",
        "        interview_data['goal']\n",
        "        + read_txt_file(interview_data['with_name_path']) + \"\"\"\n",
        "        請根據此會議逐字稿條列出此會議的重點\"\"\")\n",
        "\n",
        "    write_txt_file(interview_data['summary_path'], response.text)\n",
        "    return interview_data['with_name_path'], interview_data['summary_path']\n",
        "\n",
        "def display_results():\n",
        "    # transcript_path = interview_data['with_name_path']\n",
        "    transcript_path = interview_data['with_name_path']\n",
        "    summary_path = interview_data['summary_path']\n",
        "    with open(transcript_path, 'r', encoding='utf-8') as f:\n",
        "        transcript_content = f.read()\n",
        "\n",
        "    with open(summary_path, 'r', encoding='utf-8') as f:\n",
        "        summary_content = f.read()\n",
        "\n",
        "    return transcript_content, summary_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6lU-_e1W4g9"
      },
      "source": [
        "##UI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"<div style='font-size: 32px;'>Meeting Assistant System</div>\")\n",
        "\n",
        "\n",
        "    # 第二頁：逐字稿生成\n",
        "    with gr.Tab(\"生成逐字稿\"):\n",
        "        generate_button = gr.Button(\"轉成逐字稿\")\n",
        "        progress_output = gr.Textbox(label=\"進度\", value=\"等待生成逐字稿...\", interactive=False)\n",
        "\n",
        "        # 當點擊生成按鈕時，執行逐字稿生成並更新進度\n",
        "        def generate_transcript():\n",
        "            write_txt_file(interview_data['diarize_path'], '')\n",
        "            write_txt_file(interview_data['llmTranscript_path'], '')\n",
        "\n",
        "            # 1.語音格式轉換成wav\n",
        "            yield \"進度：格式轉換中...\"\n",
        "            convert_format(interview_data['record_path'], interview_data['wav_path'])\n",
        "            wav_audio = whisperx.load_audio(interview_data['wav_path'])\n",
        "\n",
        "            # 2.語音轉文字\n",
        "            yield \"進度：語音轉文字中...\"\n",
        "            asr_result = transcribe_audio(wav_audio)\n",
        "\n",
        "            # 3.語者標註\n",
        "            yield \"進度：語者標註中...\"\n",
        "            interview_data['speaker_list'] = diarize_audio(wav_audio, asr_result, interview_data['diarize_path'])\n",
        "            # interview_data['speaker_list'] = ['SPEAKER_00', 'SPEAKER_01', 'SPEAKER_02', 'SPEAKER_03', 'SPEAKER_04', 'SPEAKER_05', 'SPEAKER_06']\n",
        "\n",
        "            # 4.逐字稿文本處理(opencc, llm)\n",
        "            yield \"進度：逐字稿文本處理中...\"\n",
        "            process_transcript(interview_data['diarize_path'], pre_model, model, interview_data['llmTranscript_path'])\n",
        "\n",
        "            # 讀取並顯示逐字稿\n",
        "            with open(interview_data['llmTranscript_path'], 'r', encoding='utf-8') as f:\n",
        "                transcript_content = f.read()\n",
        "\n",
        "\n",
        "        generate_button.click(\n",
        "            fn=generate_transcript,\n",
        "            inputs=[],\n",
        "            outputs=[progress_output],\n",
        "            queue=True\n",
        "        )\n",
        "\n",
        "    # 第三頁：語者標註與逐字稿顯示\n",
        "    with gr.Tab(\"逐字稿與語者標註\"):\n",
        "        # 顯示逐字稿的按鈕\n",
        "        show_transcript_button = gr.Button(\"顯示逐字稿\")\n",
        "\n",
        "        # 進度顯示區域\n",
        "        progress_output = gr.Textbox(label=\"進度\", value=\"\", interactive=False, visible=False)\n",
        "\n",
        "        # 左右佈局，逐字稿和語者標註選項\n",
        "        with gr.Row(visible=False) as transcript_section:\n",
        "            # 左邊顯示逐字稿\n",
        "            transcript_output = gr.Textbox(label=\"逐字稿\", lines=15, interactive=False)\n",
        "\n",
        "            # 右邊語者標註選項區域\n",
        "            with gr.Column() as speaker_section:\n",
        "                speaker_inputs = [gr.Textbox(label=f\"語者 {i+1}\", visible=True) for i in range(10)]\n",
        "                confirm_button = gr.Button(\"確認輸入\", visible=True)\n",
        "\n",
        "        # 顯示逐字稿按鈕的行為\n",
        "        def show_transcript():\n",
        "            # 讀取逐字稿和語者列表\n",
        "            # speaker_list = ['SPEAKER_00', 'SPEAKER_01']\n",
        "            try:\n",
        "                with open(interview_data['llmTranscript_path'], 'r', encoding='utf-8') as f:\n",
        "                    transcript_content = f.read()\n",
        "            except FileNotFoundError:\n",
        "                transcript_content = \"無法找到逐字稿文件，請確認是否已完成逐字稿生成。\"\n",
        "\n",
        "            # 假設語者列表存在於 interview_data 中\n",
        "            speaker_list = interview_data.get('speaker_list', [])\n",
        "\n",
        "            # 更新輸出和顯示逐字稿的區域\n",
        "            updates = [\n",
        "                gr.update(visible=True),  # transcript_section 顯示\n",
        "                gr.update(value=transcript_content)  # transcript_output 更新為逐字稿內容\n",
        "            ]\n",
        "\n",
        "            # 顯示對應語者的輸入框以及確認按鈕\n",
        "            for i in range(len(speaker_list)):\n",
        "                updates.append(gr.update(visible=True, label=f\"語者 {i+1}（{speaker_list[i]}）\"))\n",
        "            for i in range(len(speaker_list), len(speaker_inputs)):\n",
        "                updates.append(gr.update(visible=False))  # 隱藏多餘的輸入框\n",
        "            updates.append(gr.update(visible=True))  # 確認按鈕顯示\n",
        "\n",
        "            return updates\n",
        "\n",
        "        show_transcript_button.click(\n",
        "            fn=show_transcript,\n",
        "            inputs=[],\n",
        "            outputs=[transcript_section, transcript_output] + speaker_inputs + [confirm_button]\n",
        "        )\n",
        "\n",
        "        # 當點擊確認按鈕時，替換逐字稿中的語者標籤\n",
        "        def update_speaker_names(transcript_text, *names):\n",
        "            # 建立語者映射表\n",
        "            speaker_map = {f\"SPEAKER_{i:02d}\": name for i, name in enumerate(names) if name.strip()}\n",
        "            # 更新逐字稿，替換語者名稱\n",
        "            updated_transcript = transcript_text\n",
        "            for speaker_label, name in speaker_map.items():\n",
        "                updated_transcript = updated_transcript.replace(speaker_label, name)\n",
        "\n",
        "            write_txt_file(interview_data['with_name_path'], '')\n",
        "            write_txt_file(interview_data['with_name_path'], updated_transcript)\n",
        "            return updated_transcript\n",
        "\n",
        "        confirm_button.click(\n",
        "            fn=update_speaker_names,\n",
        "            inputs=[transcript_output] + speaker_inputs,\n",
        "            outputs=[transcript_output]\n",
        "        )\n",
        "\n",
        "\n",
        "    # 第四頁：會議摘要與團隊分析\n",
        "    with gr.Tab(\"會議摘要與團隊分析\"):\n",
        "        with gr.Column():\n",
        "            # 左邊：生成會議大綱\n",
        "            summary_output = gr.Textbox(label=\"會議大綱與分析\", interactive=False)\n",
        "            summary_button = gr.Button(\"生成會議大綱與團隊分析\")\n",
        "\n",
        "            # # 右邊：團隊分析\n",
        "            # analysis_button = gr.Button(\"團隊分析\")\n",
        "            # analysis_output = gr.Textbox(label=\"團隊分析\", lines=10, interactive=False)\n",
        "\n",
        "        # 設置生成會議大綱的按鈕行為\n",
        "        def generate_summary():\n",
        "            # 模擬會議大綱生成\n",
        "\n",
        "            response = model.generate_content(\n",
        "              interview_data['goal']\n",
        "              + read_txt_file(interview_data['with_name_path']) + \"\"\"\n",
        "              請根據此會議逐字稿條列出此會議的重點\"\"\")\n",
        "\n",
        "            write_txt_file(interview_data['summary_path'], response.text)\n",
        "            summary = read_txt_file(interview_data['summary_path'])\n",
        "            return summary\n",
        "\n",
        "        summary_button.click(\n",
        "            fn=generate_summary,\n",
        "            inputs=[],\n",
        "            outputs=[summary_output]\n",
        "        )\n",
        "\n",
        "        # # 設置團隊分析的按鈕行為\n",
        "        # def generate_team_analysis():\n",
        "        #     # 模擬團隊分析生成\n",
        "        #     return \"這是團隊分析報告，包括語者的參與度和討論內容的概覽。\"\n",
        "\n",
        "        # analysis_button.click(\n",
        "        #     fn=generate_team_analysis,\n",
        "        #     inputs=[],\n",
        "        #     outputs=[analysis_output]\n",
        "        # )\n",
        "\n",
        "\n",
        "# 啟動 Gradio 應用\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "EBE7V8CHnrLw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "a28ea4bf-268a-485a-bbc5-b1c9f883f475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gr' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c004c82d2ceb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<div style='font-size: 32px;'>Meeting Assistant System</div>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mspeaker_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpre_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenai_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gr' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "-EzMj2dHHBu7"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}